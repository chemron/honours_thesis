@article{rosenblatt1958,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain},
	volume = {65},
	issn = {0033-295X},
	abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references.},
	language = {eng},
	number = {6},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	note = {Publisher: American Psychological Association},
	keywords = {Psychology},
	pages = {386--408},
	file = {Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf:/home/cam/Zotero/storage/KIKMJMHY/Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf:application/pdf}
}

@inproceedings{glorot2011,
	address = {Fort Lauderdale, FL, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Deep {Sparse} {Rectifier} {Neural} {Networks}},
	volume = {15},
	url = {http://proceedings.mlr.press/v15/glorot11a.html},
	abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training. [pdf]},
	booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	editor = {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
	month = apr,
	year = {2011},
	pages = {315--323},
	file = {Glorot et al. - 2011 - Deep Sparse Rectifier Neural Networks.pdf:/home/cam/Zotero/storage/WYEQ47SG/Glorot et al. - 2011 - Deep Sparse Rectifier Neural Networks.pdf:application/pdf}
}

@article{hodgkin1952,
	title = {A quantitative description of membrane current and its application to conduction and excitation in nerve},
	volume = {117},
	issn = {0022-3751},
	url = {https://pubmed.ncbi.nlm.nih.gov/12991237},
	doi = {10.1113/jphysiol.1952.sp004764},
	language = {eng},
	number = {4},
	journal = {The Journal of physiology},
	author = {Hodgkin, A L and Huxley, A F},
	month = aug,
	year = {1952},
	keywords = {*Axons, *AXONS, Humans},
	pages = {500--544},
	file = {HODGKIN and HUXLEY - 1952 - A quantitative description of membrane current and.pdf:/home/cam/Zotero/storage/CBB4BUJR/HODGKIN and HUXLEY - 1952 - A quantitative description of membrane current and.pdf:application/pdf}
}


@article{felipe2019,
	author = {{Felipe, T.} and {Asensio Ramos, A.}},
	title = {Improved detection of far-side solar active regions using deep learning},
	DOI= "10.1051/0004-6361/201936838",
	url= "https://doi.org/10.1051/0004-6361/201936838",
	journal = {A\&A},
	year = 2019,
	volume = 632,
	pages = "A82",
}

@Article{Kim2019,
	author={Kim, Taeyoung
	and Park, Eunsu
	and Lee, Harim
	and Moon, Yong-Jae
	and Bae, Sung-Ho
	and Lim, Daye
	and Jang, Soojeong
	and Kim, Lokwon
	and Cho, Il-Hyun
	and Choi, Myungjin
	and Cho, Kyung-Suk},
	title={Solar farside magnetograms from deep learning analysis of STEREO/EUVI data},
	journal={Nature Astronomy},
	year={2019},
	volume={3},
	number={5},
	pages={397-400},
	abstract={Solar magnetograms are important for studying solar activity and predicting space weather disturbances1. Farside magnetograms can be constructed from local helioseismology without any farside data2-4, but their quality is lower than that of typical frontside magnetograms. Here we generate farside solar magnetograms from STEREO/Extreme UltraViolet Imager (EUVI) 304-{\AA} images using a deep learning model based on conditional generative adversarial networks (cGANs). We train the model using pairs of Solar Dynamics Observatory (SDO)/Atmospheric Imaging Assembly (AIA) 304-{\AA} images and SDO/Helioseismic and Magnetic Imager (HMI) magnetograms taken from 2011 to 2017 except for September and October each year. We evaluate the model by comparing pairs of SDO/HMI magnetograms and cGAN-generated magnetograms in September and October. Our method successfully generates frontside solar magnetograms from SDO/AIA 304-{\AA} images and these are similar to those of the SDO/HMI, with Hale-patterned active regions being well replicated. Thus we can monitor the temporal evolution of magnetic fields from the farside to the frontside of the Sun using SDO/HMI and farside magnetograms generated by our model when farside extreme-ultraviolet data are available. This study presents an application of image-to-image translation based on cGANs to scientific data.},
	issn={2397-3366},
	doi={10.1038/s41550-019-0711-5},
	url={https://doi.org/10.1038/s41550-019-0711-5}
	}

@article{lecun2015,
	title = {Deep learning},
	volume = {521},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	number = {7553},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444}
}

@article{mcculloch1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	number = {4},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	pages = {115--133},
	file = {McCulloch and Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf:/home/cam/Zotero/storage/M2IKSRFK/McCulloch and Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf:application/pdf}
}

@book{michelucci2018,
	address = {Berkeley, CA},
	title = {Applied {Deep} {Learning}: {A} {Case}-{Based} {Approach} to {Understanding} {Deep} {Neural} {Networks}},
	isbn = {978-1-4842-3789-2 978-1-4842-3790-8},
	shorttitle = {Applied {Deep} {Learning}},
	url = {http://link.springer.com/10.1007/978-1-4842-3790-8},
	language = {en},
	urldate = {2020-08-04},
	publisher = {Apress},
	author = {Michelucci, Umberto},
	year = {2018},
	doi = {10.1007/978-1-4842-3790-8},
	file = {Michelucci - 2018 - Applied Deep Learning A Case-Based Approach to Un.pdf:/home/cam/Zotero/storage/65JMN7RB/Michelucci - 2018 - Applied Deep Learning A Case-Based Approach to Un.pdf:application/pdf}
}

@book{reagen2017,
	title = {Deep Learning for Computer Architects},
	volume = {12},
	isbn = {1935-3235, 1935-3243},
	url = {http://www.morganclaypool.com/doi/10.2200/S00783ED1V01Y201706CAC041},
	doi = {10.2200/S00783ED1V01Y201706CAC041},
	language = {en},
	urldate = {2020-08-04},
	publisher = {Morgan \& Claypool},
	author = {Reagen, Brandon and Adolf, Robert and Whatmough, Paul and Wei, Gu-Yeon and Brooks, David},
	month = aug,
	year = {2017},
	pages = {1--123},
	series = {},
	file = {Reagen et al. - 2017 - Deep Learning for Computer Architects.pdf:/home/cam/Zotero/storage/TZLIA4D7/Reagen et al. - 2017 - Deep Learning for Computer Architects.pdf:application/pdf}
}


@article{schmidhuber2015,
	title = "Deep learning in neural networks: An overview",
	journal = "Neural Networks",
	volume = "61",
	pages = "85 - 117",
	year = "2015",
	issn = "0893-6080",
	doi = "https://doi.org/10.1016/j.neunet.2014.09.003",
	url = "http://www.sciencedirect.com/science/article/pii/S0893608014002135",
	author = "Jürgen Schmidhuber",
	keywords = "Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation",
	abstract = "In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks."
}


@article{hubel_receptive_1959,
	title = {Receptive fields of single neurones in the cat's striate cortex},
	volume = {148},
	issn = {0022-3751},
	url = {https://pubmed.ncbi.nlm.nih.gov/14403679},
	doi = {10.1113/jphysiol.1959.sp006308},
	language = {eng},
	number = {3},
	journal = {The Journal of physiology},
	author = {Hubel, D H and Wiesel, T N},
	month = oct,
	year = {1959},
	keywords = {*CEREBRAL CORTEX/physiology, *NEURONS/physiology, *Visual Cortex, Animals, Cats, Cerebral Cortex/*physiology, Neurons/*physiology},
	pages = {574--591}
}


@article{fukushima_neocognitron_1980,
	title = {Neocognitron: {A} self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
	volume = {36},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/BF00344251},
	doi = {10.1007/BF00344251},
	abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by “learning without a teacher”, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname “neocognitron”. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of “S-cells”, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of “C-cells” similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any “teacher” during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
	number = {4},
	journal = {Biological Cybernetics},
	author = {Fukushima, Kunihiko},
	month = apr,
	year = {1980},
	pages = {193--202}
}

@article{cauchy_1847,
author="Cauchy, A.",
title="Methode generale pour la resolution des systemes d'equations simultanees",
journal="C.R. Acad. Sci. Paris",
ISSN="",
publisher="",
year="1847",
month="",
volume="25",
number="",
pages="536-538",
URL="https://ci.nii.ac.jp/naid/10026863174/en/",
DOI="",
}

@article{bryson1962steepest,
  title={A steepest-ascent method for solving optimum programming problems},
  author={Bryson, Arthur E and Denham, Walter F},
  year={1962},
  journal={Journal of Applied Mechanics}
}

@InProceedings{Werbos1982,
author="Werbos, Paul J.",
editor="Drenick, R. F.
and Kozin, F.",
title="Applications of advances in nonlinear sensitivity analysis",
booktitle="System Modeling and Optimization",
year="1982",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="762--770",
abstract="The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost. The areas of application include the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling. The details, references and derivations can be obtained by requesting „Sensitivity Analysis Methods for Nonlinear Systems`` from Forecast Analysis and Evaluation Team, Quality Assurance, OSS/EIA, Room 7413, Department of Energy, Washington, DC 20461.",
isbn="978-3-540-39459-4"
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	number = {6088},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	pages = {533--536}
}

@InProceedings{Bottou2010,
author="Bottou, L{\'e}on",
editor="Lechevallier, Yves
and Saporta, Gilbert",
title="Large-Scale Machine Learning with Stochastic Gradient Descent",
booktitle="Proceedings of COMPSTAT'2010",
year="2010",
publisher="Physica-Verlag HD",
address="Heidelberg",
pages="177--186",
abstract="During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.",
isbn="978-3-7908-2604-3"
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@incollection{Goodfellow2014,
title = {Generative Adversarial Nets},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {2672--2680},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}
}

@ARTICLE{Hinton2012,
  author={G. {Hinton} and L. {Deng} and D. {Yu} and G. E. {Dahl} and A. {Mohamed} and N. {Jaitly} and A. {Senior} and V. {Vanhoucke} and P. {Nguyen} and T. N. {Sainath} and B. {Kingsbury}},
  journal={IEEE Signal Processing Magazine}, 
  title={Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups}, 
  year={2012},
  volume={29},
  number={6},
  pages={82-97}
}

@incollection{Krizhevsky2012,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}


@inproceedings{isola2017image,
  title={Image-to-image translation with conditional adversarial networks},
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1125--1134},
  year={2017}
}

@article{Solanki_2006,
	doi = {10.1088/0034-4885/69/3/r02},
	url = {https://doi.org/10.1088%2F0034-4885%2F69%2F3%2Fr02},
	year = 2006,
	month = {feb},
	publisher = {{IOP} Publishing},
	volume = {69},
	number = {3},
	pages = {563--668},
	author = {Sami K Solanki and Bernd Inhester and Manfred Schüssler},
	title = {The solar magnetic field},
	journal = {Reports on Progress in Physics},
	abstract = {The magnetic field of the Sun is the underlying cause of the many diverse phenomena combined under the heading of solar activity. Here we describe the magnetic field as it threads its way from the bottom of the convection zone, where it is built up by the solar dynamo, to the solar surface, where it manifests itself in the form of sunspots and faculae, and beyond into the outer solar atmosphere and, finally, into the heliosphere. On the way it transports energy from the surface and the subsurface layers into the solar corona, where it heats the gas and accelerates the solar wind.}
}


@misc{nasa_stereo,
	title = {{STEREO} {A} {Spacecraft}},
	url =	{https://nssdc.gsfc.nasa.gov/nmc/spacecraft/display.action?id=2006-047A},
    year = {2006},
	urldate = {2020-07-30},
	author = {NASA},
	file = {NASA - NSSDCA - Spacecraft - Details:/home/cam/Zotero/storage/665AFA47/display.html:text/html}
}


@misc{nasa_sdo,
	title = {{SDO} {Spacecraft}},
	url = {https://nssdc.gsfc.nasa.gov/nmc/spacecraft/display.action?id=2010-005A},
	urldate = {2020-07-30},
	year = {2010},
	author = {NASA},
	file = {NASA - NSSDCA - Spacecraft - Details:/home/cam/Zotero/storage/BQ3U7LZW/display.html:text/html}
}


@article{beck_comparison_2000,
	title = {A comparison of differential rotation measurements – ({Invited} {Review})},
	volume = {191},
	issn = {1573-093X},
	url = {https://doi.org/10.1023/A:1005226402796},
	doi = {10.1023/A:1005226402796},
	abstract = {Observers have long measured solar rotation with different techniques and obtained different results. This paper compares differential rotation measurements from four techniques: Doppler shift, Doppler feature tracking, magnetic feature tracking, and p-mode splittings. The different rotation rates measured by the first three techniques are interpreted as rotation rates of solar phenomena which depend on the properties and depth of that which is measured. This interpretation is supported by comparison with rotation measurements obtained from p-mode splittings except for Doppler features. The rotation rate of the plasma corresponds to the surface rate obtained by inversions; the rates of magnetic features correspond to the rotation rate at various depths within the convection zone. Supergranulation rotates at a rate greater than the maximum rotation rate within the convection zone, suggesting that supergranules are not simple convection cells anchored at a particular depth.},
	number = {1},
	journal = {Solar Physics},
	author = {Beck, John G.},
	month = jan,
	year = {2000},
	pages = {47--70}
}

@INPROCEEDINGS{Alshehhi2020,
	author={R. {Alshehhi}},
	booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
	title={Deep Regression for Imaging Solar Magnetograms using Pyramid Generative Adversarial Networks},
	year={2020},
	volume={},
	number={},
	pages={807-815}
}

@book{hughes2007solar,
  title={The solar tachocline},
  author={Hughes, David Wolstenholme and Rosner, Robert and Weiss, Nigel Oscar},
  year={2007},
  publisher={Cambridge University Press}
}

@article{kristiansen2010nebra,
  title={The Nebra find and early Indo-European religion},
  author={Kristiansen, Kristian},
  journal={Der Grif nach den Sternen. Wie Europas Eliten zu Macht und Reichtum kamen},
  volume={2},
  pages={431--7},
  year={2010}
}

@article{Mino2018,
  title={LoGAN: Generating Logos with a Generative Adversarial Neural Network Conditioned on Color},
  author={Ajkel Mino and Gerasimos Spanakis},
  journal={2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)},
  year={2018},
  pages={965-970}
}



@article{mirza_conditional_2014,
	title = {Conditional {Generative} {Adversarial} {Nets}},
	journal = {arXiv e-prints},
	author = {Mirza, Mehdi and Osindero, Simon},
	month = nov,
	year = {2014},
	note = {\_eprint: 1411.1784},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {arXiv:1411.1784}
}

@article{boerner_photometric_2014,
	title = {Photometric and {Thermal} {Cross}-calibration of {Solar} {EUV} {Instruments}},
	volume = {289},
	doi = {10.1007/s11207-013-0452-z},
	number = {6},
	journal = {{\textbackslash}solphys},
	author = {Boerner, P. F. and Testa, P. and Warren, H. and Weber, M. A. and Schrijver, C. J.},
	month = jun,
	year = {2014},
	note = {\_eprint: 1307.8045},
	keywords = {Astrophysics - Solar and Stellar Astrophysics, Atomic data, Chromosphere, Corona, EUV, Instrumentation, Transition region},
	pages = {2377--2397}
}


@article{toth_phone_2015,
	title = {Phone recognition with hierarchical convolutional deep maxout networks},
	volume = {2015},
	issn = {1687-4722},
	url = {https://doi.org/10.1186/s13636-015-0068-3},
	doi = {10.1186/s13636-015-0068-3},
	abstract = {Deep convolutional neural networks (CNNs) have recently been shown to outperform fully connected deep neural networks (DNNs) both on low-resource and on large-scale speech tasks. Experiments indicate that convolutional networks can attain a 10–15 \% relative improvement in the word error rate of large vocabulary recognition tasks over fully connected deep networks. Here, we explore some refinements to CNNs that have not been pursued by other authors. First, the CNN papers published up till now used sigmoid or rectified linear (ReLU) neurons. We will experiment with the maxout activation function proposed recently, which has been shown to outperform the rectifier activation function in fully connected DNNs. We will show that the pooling operation of CNNs and the maxout function are closely related, and so the two technologies can be readily combined to build convolutional maxout networks.},
	number = {1},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Tóth, László},
	month = sep,
	year = {2015},
	pages = {25}
}


@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2020-11-27},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/home/cam/Zotero/storage/2I95CD2J/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf}
}


@inproceedings{ronneberger_u-net_2015,
	title = {U-net: {Convolutional} networks for biomedical image segmentation},
	booktitle = {International {Conference} on {Medical} image computing and computer-assisted intervention},
	publisher = {Springer},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	year = {2015},
	pages = {234--241},
}


@article{kingma_adam_2014,
	title = {Adam: {A} method for stochastic optimization},
	journal = {arXiv preprint arXiv:1412.6980},
	author = {Kingma, Diederik P and Ba, Jimmy},
	year = {2014},
}

