\documentclass[11pt,a4paper,onecolumn]{report}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{lmodern}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\usepackage{bm}
\usepackage[margin=2cm]{geometry}
\usepackage{array}
\usepackage{physics}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage[T1]{fontenc}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{enumerate}
\pagestyle{fancy}
\setlength{\headheight}{13.6pt}

\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, calc}
\usepackage[nottoc, numbib]{tocbibind}
\tikzstyle{cool} = [rectangle, rounded corners, minimum width=3cm, minimum
height=1cm, text centered, draw=black, fill=gray!30, text width=3cm]
\tikzstyle{arrow} = [thick, ->, >=stealth] \tikzstyle{line} = [thick, -,
>=stealth]

\usepackage[detect-all]{siunitx}
\usepackage{dsfont}
\usepackage{breqn}
\usepackage{subfigure}
\usepackage{geometry}
\usepackage{listings}
\usepackage{caption}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage[square,numbers,comma,sort&compress]{natbib}
\usepackage{upgreek}
\usepackage{aas_macros}
\usepackage{doi}
\usepackage{siunitx}
\usepackage{textgreek}


\geometry{a4paper, left=25mm, top=25mm,} % total={160mm,247mm} <- overspecification
\graphicspath{{figures/} }
\renewcommand{\listfigurename}{Figures}


\captionsetup{justification   = raggedright, singlelinecheck = false}

\hypersetup{colorlinks, citecolor=black, filecolor=black, linkcolor=black,
    urlcolor=black}

\renewcommand{\bibname}{References}

\bibpunct{(}{)}{;}{a}{}{,}

\hypersetup{colorlinks, citecolor=black, filecolor=black, linkcolor=black,
    urlcolor=black}

% table of contents:
\setcounter{tocdepth}{1}

\newcommand*\chem[1]{\ensuremath{\mathrm{#1}}}

\newcommand{\threevdots}{%
  \vbox{\baselineskip1ex\lineskiplimit0pt%
  \hbox{.}\hbox{.}\hbox{.}}}



% diagnose: Label(s) may have changed. Rerun to get cross-references right.
% \def\@testdef #1#2#3{% \def\reserved@a{#3}\expandafter \ifx \csname
% #1@#2\endcsname \reserved@a  \else \typeout{^^Jlabel #2 changed:^^J%
% \meaning\reserved@a^^J% \expandafter\meaning\csname #1@#2\endcsname^^J}%
% \@tempswatrue \fi}


%opening
% \title{Predicting the Magnetic Field of the Solar Farside}
% \author{Cameron Smith\\
% Student ID: 28792912\\
% Supervisors: Andrew Casey, Alina Donea}
% \date{\today}



\begin{document}


\begin{titlepage}
  \begin{center}
    \vspace*{2cm}
    \Huge
    \textbf{Predicting the Magnetic Field of the Solar Farside}

    \vspace{2cm}
    \LARGE
    \textbf{Cameron Smith}

    \vspace{0.8cm}
    \Large \textit{Supervisors:}\\
    Andrew Casey\\
    Alina Donea

    \vfill
    \large
    An honours thesis presented for the degree of\\
    Bachelor of Science Advanced - Research (Honours)

    \vspace{0.3cm}
    \includegraphics[width=0.2\linewidth]{"Monash_Logo"}\\
    School of Physics and Astronomy\\
    Faculty of Science\\
    Monash University\\
    Australia

    \vspace{0.5cm}

    \today

  \end{center}
\end{titlepage}

\chapter*{Abstract}
Active regions on the surface of the Sun can lead to large eruptions that have
the potential to cause significant hazards on the Earth. Images of the magnetic
field on the Sun's surface, known as magnetograms, are a vital tool for
predicting these events. While these are readily available for the Sun's
hemisphere that faces the Earth, there is currently no reliable way to
generate magnetograms for the opposite hemisphere. Consequently, potentially
dangerous farside active regions may rotate into the view of Earth with little
warning. Here we describe a method to generate synthetic farside magnetograms
from a helioseismology data product using a deep learning method. While the
synthetic magnetograms are unable to accurately predict the position or shape of
active regions, they successfully predict sharp changes in the total unsigned
magnetic field - a key predictor of solar eruptions.


% \chapter*{Acknowledgements}
%TODO:

% Andy and Alina for their support and invaluable feedback
% Bernhard being very accomadating and willing to help/spend time and effort to
% ensure we weren't adversaly effected by lockdowns/etc

% Friends and family and lovely girlfriend for keeping me sane, and making my
% offtime so much more enjoyable

% Everyone mentioned a huge contribution into making the past year and
% especially the past semester much more pleasant


\tableofcontents


% \listoffigures







%
%
%
%
\chapter{Introduction}
%
%
%
%

Active regions are areas of intense magnetic field on the surface of the Sun.
These are known to cause significant eruptions, such as coronal mass ejections
or solar flares. These extreme events can eject fast-moving particles into
interstellar space which have the potential to cause mass electrical blackouts,
loss of satellites, and hazardous levels of radiation exposure for astronauts
\citep{Mewaldt2005,council_severe_2008}. \\

Solar magnetograms, images of the magnetic field of the Sun, can be used to
identify potentially high-risk active regions before they erupt
\citep{bobra_solar_2015}. However, these magnetograms are only available for the
`nearside': the hemisphere of the Sun that faces the Earth. Due to the rotation of
the Sun, active regions that emerge on the `farside' could be facing the Earth
only seven days after becoming visible. A method for imaging the farside
regions is necessary to identify dangerous farside magnetic activity before it
reaches Earth. \\

Using a conditional generative adversarial network (cGAN), a deep learning
technique useful for image-to-image translation, \citet{Kim2019} were able to
create magnetograms based on extreme ultraviolet (EUV) images taken by NASA's
Solar Terrestrial Relations Observatory (STEREO)\footnote{STEREO actually
consists of two spacecraft, STEREO ahead (A) and STEREO behind (B), however,
contact with STEREO-B was lost in 2016, and so the images used were taken by
STEREO-A.}. The orbit of STEREO-A allows it to move around the Sun relative to
the Earth, and take EUV images on the farside during some points of its orbit,
which can then be used to generate farside magnetograms. However STEREO-A
currently only has a partial view of the farside, a view that will only decrease
until it passes the Earth in 2023. By then, STEREO-A will be more than 16 years
into its planned 2-year mission and may have lost contact with the Earth.
Futhermore \citet{Kim2019} used a saturation point of 100 Gauss when generating
these magnetograms. As many active regions have magnetic fields stronger than
1000 G, this severely limits the ability to predict extreme events.\\


To avoid these issues, we instead employ a method to generate synthetic
magnetograms from farside seismic maps - images that measure perturbations on
the far hemisphere by computing the travel-time of acoustic waves, known as
p-modes, that travel through the interior of the Sun \citep{Lindsey2017}. This
generation is done using a cGAN similar to that used by \citet{Kim2019}, trained
on 4288 farside seismic map/magnetogram pairs. 


As the
precise relationship between these seismic maps and the magnetic field is
unknown, a cGAN similar to that used by \citet{Kim2019} is first created to
generate farside magnetograms from seismic maps, therefore allowing earlier
detection of dangerous active regions.


%TODO: finish Introduction





%
%
%
%
%
%
\chapter{Background}
%
%
%
%
%
%


%
%
%
% Section: The Sun
%
%
%

\section{The Sun}
\label{sec:Sun}
% Formation of the Sun

The formation of the Sun began around 4.6 billion years ago, with a giant
molecular gas cloud approximately 65 light-years wide
\citep{montmerle_solar_2006}, which consisted of predominantly hydrogen, as well
as helium and trace amounts of lithium. \\

If one such cloud reaches a critical mass, the internal gas pressure will be
unable to continue supporting it, causing the cloud to undergo gravitational
collapse \citep{jeans_stability_1902}. This collapse leads to the formation of
potentially thousands of stars. Under the right conditions, massive (\(\gtrsim
\SI{9}{\,M_\odot} \)), short-lived stars in this cluster may explode as
supernovae \citep{heger_how_2003}, sending a shock through the molecular cloud
at high speeds. This can trigger the creation of more stars, which may go on to
also produce supernovae, giving rise to self-propagating star formation
\citep{mueller_propagating_1976}. The Solar System itself was likely formed in
this process, as part of a since dispersed cluster with a mass of around
\(\SI{3000}{\,M_\odot} \)\citep{williams_astrophysical_2010,zwart_lost_2009}. \\

As the Sun-forming fragment of molecular cloud collapses, it spins faster due to
the conservation of angular momentum. The molecules within begin colliding at an
increasing frequency, converting some of their kinetic energy to heat. The
centre of this collapsing nebula collects the majority of the mass to become an
increasingly hot and dense protostar, while the surrounding nebula flattens into
a protoplanetary disc. This mass becomes the building material for the solar
system, with the planets forming from the protoplanetary disk
\citep{greaves_disks_2005}. \\

As the Sun continues to contract, the temperature and pressure in the core
increases, eventually leading to fusion, at which point the Sun reaches it's
current stage of life as a main-sequence star \citep{woolfson_origin_2000}.
Proton-proton chain reaction (\textit{pp} chain) dominates this fusion process,
accounting for approximately $99\%$ of the Sun's energy, while the CNO cycle
generates the remaining \(\sim 1\%\) of the energy \citep{adelberger_solar_2011}.
The \textit{pp} chain process can be summarised as
\begin{align}
  \chem{p} + \chem{p} &\rightarrow \chem{\prescript{2}{}H} + \chem{e^+} +
  \nu_e \label{eqn:pp1}\\
  \chem{\prescript{2}{}H} + \chem{p} &\rightarrow \chem{\prescript{3}{}He}
  + \gamma \label{eqn:pp2}\\
  \chem{\prescript{3}{}He} + \chem{\prescript{3}{}He} & \rightarrow
  \chem{\prescript{4}{}He} + \chem{p} + \chem{p}\,, \label{eqn:pp3}
\end{align}
where Equations \ref{eqn:pp1} and \ref{eqn:pp2} must each occur twice to create
enough $\chem{\prescript{3}{}He}$ for Equation \ref{eqn:pp3} to occur. The
energy released by the fusion process comes in the form of gamma-ray photons,
heating the Sun from the inside, giving rise to the luminous hot ball of plasma
that we observe today. \\


Observation of the Sun is typically done through the use of either ground or
space-based telescopes. The Sun's atmosphere can be imaged at a range of depths
by viewing the Sun at different wavelengths, due to the variation in the Sun's
temperature and composition. Of particular note in this thesis is light with a
wavelength of \(\SI{304}{\angstrom}\), which is emitted in the chromosphere (the
layer of atmosphere between the photosphere and corona) by \(\chem{He \, II}\)
at a temperature of around \(\SI{50000}{\kelvin}\)
\citep{herbert_friedman_solar_1962}. \\

While electromagnetic radiation is effective for imaging the solar atmosphere,
past the photosphere (the Sun's visual surface, and henceforth referred to as
the surface) the Sun is no longer transparent to light, and so information about
the nature of the Sun below this surface must be inferred indirectly. \\


%
\subsection{Helioseismology}
%
\label{sec:HSM}

In \citeyear{leighton_velocity_1962}, \citeauthor{leighton_velocity_1962}
noticed oscillations of the Sun's surface varied with a period of \(\sim
5\) minutes. While initially assumed to be surface flows from solar granules,
further work found that the observed motion was due to the superposition of
resonant modes of oscillation in the Sun \citep{ulrich_five-minute_1970}. These
oscillations were later found to be a surface signature of pressure-modes
(p-modes) \citep{deubner_observations_1975}; standing waves generated by the
turbulent convective motion a few hundred kilometres below the surface. Pressure
is the dominant restoring force of p-modes (hence the name), effectively making
them sound waves (albeit at a far lower frequency than what is audible), with
frequencies ranging between 1 and 5 mHz. Unless propagating exactly radially,
these acoustic waves are continuously refracted as they travel deeper into the
Sun due to the changing speed of sound, eventually making their way back to the
surface. When they reach the surface, they are reflected back towards the
centre, effectively trapping them in a resonating cavity. \\

In addition to p-modes, two other types of modes exist in the Sun: gravity-modes
(g-modes) and surface gravity modes (f-modes). G-modes are confined to the
convectively stable radiative zone (from the core to a radius of $\SI{0.7}
{R_\odot}$), with buoyancy as the restoring force. F-modes also have buoyancy as
their restoring force, but instead travel along the surface of the Sun. \\

Each mode can be characterised by three quantum numbers: the radial order,
\(n\), the angular degree \(l\) (\(l \geq 0\)), and the azimuthal order m (\(-l
\leq m \leq l\)). Each mode has a resonant frequency \(\omega_{nlm}\) which
increases monotonically with \(n\), and can be measured by taking a Fourier
transform of the observed oscillations. In spherically symmetric conditions, the
frequencies of these modes would be independent of \(m\), however, this is not
observed, with the internal rotation of the Sun breaking this symmetry. P-modes
make up the high-frequency modes (with \(n>0\)), while g-modes make up the
low-frequency modes (with \(n<0\)). F-modes are then the intermediate mode, with
\(n=0\). Figure \ref{fig:hsm_power} shows a power spectrum of the p-mode
oscillations, as a function of frequency and angular degree. \\

\begin{figure}[t]
  \centering
  \subfigure[]{
    \includegraphics[height = 2in]{hsm_power_spectrum.png}
    \label{fig:hsm_power}
  }
  \subfigure[]{
    \includegraphics[height = 2in]{sound_speed.jpg}
    \label{fig:sound_speed}
  } \caption{\subref{fig:hsm_power} Power spectrum of the Sun's p-mode
  oscillations. Each ridge corresponds to a different value of \(n\).
  \textit{Image by Warrick Ball, using data from the Michelson Doppler Imager
  (MDI) aboard the Solar and Heliospheric Observatory (SOHO). Distributed under
  a CC BY-SA 4.0 license}. \subref{fig:sound_speed} The speed of sound
  (\(\si{km\per s}\)) inside the Sun as a function of the fractional radius
  (\(\si{r \per R_\odot}\)) \citep{bahcall_solar_2000}.}

\end{figure}

These modes are influenced by the structure and gravity inside the Sun, as well
as the large scale flows and magnetic fields. When the perturbations from these
resonating waves reach the surface of the Sun, the motion creates surface
oscillations, while the local change in pressure causes a fluctuation in the
temperature. Detecting the modes can therefore be achieved by observing either the
luminosity or the Doppler shift.\\

The goal of helioseismology is to observe these modes and deduce the causal
factors that influence them, thereby obtaining information about the solar
interior. While g-modes offer the potential to probe the inner core of the Sun,
the amplitude of the resulting perturbations will be very low by the time they
reach the surface. Consequently, the detection of g-modes has so far proved
elusive \citep{appourchaux_quest_2010}, with only a few possible exceptions (for
example, \citealp{fossat_asymptotic_2017}). P-modes on the other hand, while
unable to probe as deep into the Sun, have proven much easier to observe and
examine \citep{deubner_observations_1975}. \\

By measuring the frequency of p-modes in the Sun, helioseismology can be used to determine
the speed of sound as a function of the radius, \(c\left(r\right)\)
\citep{christensen-dalsgaard_speed_1985}, which can in itself be used to
determine the temperature as a function of the radius, \(T\left(r\right)\), due
to the relationship
\begin{align}
  c^2 = \frac{\bar{R}\Gamma_1 T}{\mu} \,,
\end{align}
where \(\bar{R}\) is the gas constant, \(\Gamma_1\) is the adiabatic exponent,
and \(\mu\) is the mean molecular weight. The `bump' seen in Figure
\ref{fig:sound_speed} around \(\SI{0.7}{r \per R_\odot}\) indicates the point where the
Sun becomes convectively unstable, and the dominant form of energy transport
transitions from radiation to convection, allowing helioseismology to determine
the precise depth of the convective zone
\citep{christensen-dalsgaard_speed_1985}. Similarly, the dip in the sound speed
at the centre of the Sun is a signature of the fusion in the core, which gives
insight into both the current fusion process and the history of nuclear
reactions. \\

% In the convective zone, the gradient of the temperature (and therefore speed of
% sound) is influenced by the opacity of the plasma to radiation, 


% rotation of the Sun

From the frequency splittings of p-modes, it is possible to determine the
angular velocity of the Sun as a function of radius and latitude
\citep{schou_helioseismic_1998}. Figure \ref{fig:internal_rotation} shows the
result of such a process, based on Doppler data from the Michelson Doppler
Imager (MDI) aboard the Solar and Heliospheric Observatory (SOHO). From this
process, it is now known that the convective zone is differentially rotating
with rotation rates that vary with latitude \citep{eff2012dynamics}. Within the
convective zone, the period of the rotation is approximately 25 days at the
equator and approximately 35 days near the poles \citep{hughes2007solar}. This
is in agreement with surface measurements of the rotation based on the motion of
sunspots across the Sun \citep{schou_helioseismic_1998}. \\


% tachocline
Beneath the convection zone in the radiative zone and core, the Sun appears to
exhibit almost solid-body rotation. However, the uncertainties on these
measurements become much greater towards the core. There is a thin layer
(\(\sim \SI{28000}{km}\) thick) separating the convective and radiative zone which
experiences a large shear due to the rapid change of rotation
\citep{spiegel1992}. This transition region is called the tachocline and is
widely thought to be the location where the Sun's large scale magnetic fields are
generated by the solar dynamo.

\begin{figure}[t]%
  \centering
  \subfigure[]{%
    \label{fig:internal_rotation}%
    \includegraphics[height=2in]{internal_rotation.png}% 
  }%
  \qquad
  \subfigure[]{%
    \includegraphics[height=2in]{tacholine_rotation.png}%
    \label{fig:tacholine_rotation}%
  }%
  \caption[]{\subref{fig:internal_rotation} The rotation of the convective zone
    in the Sun, as inferred by global helioseismology
    \citep{thompson_helioseismology_2004}. \subref{fig:tacholine_rotation} The
    rotation rate as a function of latitude and depth, showing differential
    rotation in the convective zone and nearly uniform rotation in the radiative
    zone. \textit{Image Courtesy of Global Oscillation Network Group
    (GONG)\footnotemark }.}
  \label{fig:solar_rotation}
\end{figure}



\subsection{The Solar Magnetic field}
%
\label{sec:dynamo}
%TODO: make sure this lines up
\footnotetext{\url{gong.nso.edu}}

By using the Zeeman effect---the splittings of the spectral lines in the presence of
a strong magnetic field---we can observe magnetic fields on the surface of
the Sun \citep{zeeman_over_1896}. This was first done by \citet{hale_probable_1908}, who noticed the
intense magnetic fields of sunspots, and presently can be used to make full-disk
solar magnetograms, such as the one shown in Figure \ref{fig:hmi}.
\\
\begin{figure}[t]
  \centering
  \includegraphics[width=0.4\linewidth]{hmi.jpg}
  \caption{Magnetogram taken by the Solar Dynamics Observatory's Helioseismic
  Magnetic Imager on 17 November, 2014. \textit{Image courtesy of NASA}.}
  \label{fig:hmi}
\end{figure}


These sunspots are now known to be surface manifestations of a large scale solar
magnetic field consisting of a poloidal (north-south) and toroidal (east-west)
component originating inside the Sun. \citet{JosephLarmor1919} suggested that
these large scale magnetic fields are generated by the inductive motion of the highly
conductive plasma, as part of a solar `dynamo'. For such a dynamo to exist, it
must convert the kinetic energy of the differentially rotating plasma into a
self-regenerating magnetic field, with the poloidal component somehow creating
and strengthening the toroidal component and vice versa. \\



While there are currently many dynamo theories (see
\citealt{charbonneau_dynamo_2020} for example), there is currently no consensus as
to the exact mechanism of the dynamo. Perhaps the biggest clue for finding a
dynamo model comes from sunspot observations. A successful dynamo model must be
able to replicate the almost 400 years of scientific observations from
\citet{galilei_sunspots_2010} to the present day. Importantly, such a model
must account for the following phenomena:

\begin{enumerate}
  \item Sunspot activity takes place over 11 year `solar cycles', where the size
  and number of sunspots on rises to a `solar maximum', then falls to a `solar
  minimum' \citep{schwabe_astronomische_1844}. Figure \ref{fig:sunspot_area}
  shows this solar cycle over the last 400 years, including the `Maunder
  Minimum', a period of around 70 years which saw very few sunspots.

  \item As can be seen in Figure \ref{fig:butterfly diagram}, the location of sunspot
  formation is restricted to two latitudinal bands approximately \(30\degree\)
  wide, mirrored each side of the equator. These bands converge toward the equator
  over the course of the solar cycle, ultimately covering around \(\pm 15\degree\)
  in latitude before starting over again in the next cycle
  \citep{carrington_observations_1863}. This is known as Sp\"orer's law.

  \item Sunspots tend to form in pairs of opposite polarity. Over the course of the
  solar cycle, the polarity of the leading sunspots of each pair (with respect
  to the rotation of the Sun) is typically the same across the hemisphere, and
  opposite to the leading sunspots in the opposite hemisphere
  \citep{hale_law_1925}. For example, in solar cycle 24 (2008 to 2019) leading
  sunspots typically had a negative polarity in the northern hemisphere and a
  positive polarity in the southern hemisphere, while in solar cycle 25, this is
  reversed. This is known as Hale's law and is shown in Figure
  \ref{fig:mag_butterfly}, over the duration of four solar cycles.

  \item Large sunspot pairs often emerge with a systematic tilt, with the
  leading sunspot closer to the equator than the trailing sunspot
  \citep{hale_magnetic_1919}. This is known as Joy's law.
\end{enumerate}


\begin{figure}[t]%
  \centering
  \subfigure[]{%
    \includegraphics[width=0.6 \linewidth]{sunspot_area.png}%
    \label{fig:sunspot_area}%
  }\\%
  \subfigure[]{%
    \label{fig:butterfly diagram}%
    \includegraphics[width= 0.8\linewidth]{ButterflyDiagram.png}%
  }\\%
  \subfigure[]{%
  \includegraphics[width=0.8\linewidth]{magbfly.png}%
  \label{fig:mag_butterfly}%
  }%
  \caption[]{\subref{fig:sunspot_area} The number of sunspots observed on the
  solar surface as a function of time, over the course of the past 400 years.
  \textit{Image by Robert A. Rohde, as part of the Global Warming Art project.
  distributed under a CC BY-SA 3.0 license}. \subref{fig:butterfly diagram} A
  `Butterfly diagram' of the Sun, showing the evolution of sunspots over the
  course of many solar cycles. \textit{Image courtesy of NASA}.
  \subref{fig:mag_butterfly} A Butterfly diagram of the Sun, this time showing
  the magnetic field of the sunspots. \textit{Image courtesy of NASA}. }
\end{figure}

Furthermore, as can be seen in Figure \ref{fig:mag_butterfly} near the poles,
the sign of the poloidal magnetic field flips in the middle of each solar cycle,
near the point of maximum solar activity, while the sign of the toroidal field
flips between each cycle, as indicated by Hales law. As such, the solar dynamo
must complete a full cycle over the course of 22 years (two solar cycles), with the
poloidal (\(P\)) and toroidal (\(T\)) fields been generated as follows:
\begin{align}
  P^+ \rightarrow T^- \rightarrow P^- \rightarrow T^+ \rightarrow P^+ \rightarrow \dotsc\,,
  \label{eqn:dynamo process}
\end{align}
where (\(^+\)) and (\( ^-\)) are the signs of the magnetic fields. \\

Putting even more constraints on a dynamo model, \citet{cowling1933} showed that
`An axis-symmetric magnetic field cannot be maintained by dynamo action'.
Subsequent `antidynamo' theorems by \citet{backus1956} and others have
concluded that a dynamo powering the Sun's magnetic field must not possess a
high degree of symmetry and so necessarily must be the result of a more complex
mechanism.\\


To find such a mechanism, we require an understanding of the interplay between the
motion of the highly conductive plasma and the changing magnetic field.
Magnetohydrodynamics gives us this necessary insight by combining the
equations of fluid dynamics with that of electromagnetism. Perhaps the principle
equation of magnetohydrodynamics is the ideal induction equation, which can be expressed as
\begin{align}
  \partialderivative{\bm{B}}{t} &=
  \nabla \crossproduct \left(\bm{v}\crossproduct \bm{B}\right)\,.
  \label{eqn:ideal_induction}
\end{align}
% A full derivation of the induction equation is shown in Appendix
% \ref{sec:induction_eqn}.
Any successful dynamo theory must therefore provide a
velocity field, \(\bm{v}\), and a magnetic field, \(\bm{B}\), that satisfies
this equation. \\

Using the induction equation it can be shown that in the limit of infinite
electrical conductivity, magnetic field lines are `frozen' into the Sun's plasma
and must move along with it \citep{Alfven1943}. The consequence of this is a
continuous struggle between the magnetic field and flow of the plasma, where
strong magnetic fields will pull on the plasma, while strong currents will pull
on the magnetic field. These magnetic field lines may therefore organise into
`flux tubes': cylindrical boundaries along magnetic field lines that move with
the plasma. \\

As depicted in Figure \ref{fig:dynamo}(b), because of this effect, as well as
the differential rotation of the Sun, the plasma pulls on initially poloidal
field lines more strongly the closer they are to the equator. After many
rotations, this results in the twisting of the poloidal field lines into
toroidal ones (Figure \ref{fig:dynamo}c). This process is called the
\textomega-effect. The depth where this mechanism occurs is subject to some
debate, with dynamo theories placing it in either the tachocline (for example
\citealt{deluca_dynamo_1988}) or the convective zone (for example
\citealp{chen_emergence_2017}). The \textomega-effect accounts for the first
half of the dynamo mechanism (\(P \rightarrow T\)) and is relatively well
understood.\\

%
While the mechanism for generating a poloidal field from a toroidal field (\(T
\rightarrow P\)) is much more contentious, it is very likely tied to the
formation and evolution of sunspots. The current leading model of sunspot
formation was first introduced by \citet{parker_formation_1955}. In this model,
a toroidal flux `rope' consisting of many individual flux tubes becomes buoyant
and rises to the surface of the Sun. The balance of pressures inside and outside
this flux rope is given by,
\begin{align}
  P_{B,i} + P_{G,i} &= P_{G,e}\,,
\end{align}
where \(P_{B,i}\) is the internal magnetic pressure, \(P_{G,i}\) is the internal
gas pressure and \(P_{G,e}\) is the external gas pressure. By definition of the
respective pressures, this can be formulated in terms of the densities as
follows:
\begin{align}
  \label{eqn:density balance}
  \frac{\bm{B}^2}{2\mu_0} + \rho_i \frac{k_B T_i}{\mu} &= \rho_e \frac{k_B T_e}{\mu}\,,
\end{align}
where \(\mu_0 \) is the magnetic permeability, \(k_B\) is Boltzmann's constant,
\(\mu\) is the mean molecular weight, \(\rho_i\) and \(\rho_e\) are the internal
and external density, and \(T_i\) and \(T_e\) are the internal and external
temperatures.\\

\noindent As the first term on the right-hand side of Equation \ref{eqn:density balance}
is always positive, we have:
\begin{align}
  \label{eqn:density_temp}
  \rho_i T_i &< \rho_e T_e\,.
\end{align}
If the internal and external temperatures are equal, the internal density will
be less than the external density. Furthermore, if the flux rope is rising through
the Sun, it will typically have a higher temperature than its surroundings as
it comes from a hotter region of the Sun. Therefore, Equation
\ref{eqn:density_temp} reduces to:
\begin{align}
  \rho_i < \rho_e\,,
\end{align}
and so the flux rope will experience an upward buoyancy force. This
buoyancy force competes with a magnetic tension in the flux rope, causing it to
stretch as rises. \\

As such a flux rope begins to rise, it may begin to twist due to
cork-screw-shaped `cyclonic' vortices in the turbulent flow in the convective
region \citep{parker_formation_1955}. The Coriolis effect causes vortices in the
northern hemisphere to spin in the opposite direction to those in the southern
hemisphere due to the rotation of the Sun, analogous to how cyclones behave on
the Earth. These twisting flux ropes would break off from the toroidal field,
and form loops in the meridional plane as can be seen in Figure
\ref{fig:dynamo}(c). Importantly, this process would break the axis symmetry
prohibited by the aforementioned antidynamo theorems, and also explain the
equatorial tilt that constitutes Joy's law. The net effect of these loops around
the Sun would be to create a toroidal current according to Ampere's law, which
in turn would contribute to the large scale poloidal magnetic field. This
process was first introduced by \citet{parker_hydromagnetic_1955} and comprises
the \textalpha-effect. In principle, a combination of the \textalpha-effect and
the \textomega-effect can complete the dynamo process shown in Equation
\ref{eqn:dynamo process}. As the flux rope breaks the surface, it forms an
`\(\Omega\)-loop' and creates an active region with sunspots of opposite
polarity at each entry point. \\

While the stability and rise of these flux ropes is now reasonably well
understood, the process in which the large scale magnetic field produces the
necessarily concentrated toroidal flux ropes remains unknown. However at the
very least, if we assume that sunspots rise approximately radially and that
a stronger toroidal field generates stronger and more frequent sunspots, the
location and strength of sunspots can provide a map of the toroidal magnetic
field. Under this assumption, `Butterfly' diagrams, such as the ones shown in
Figures \ref{fig:butterfly diagram} and \ref{fig:mag_butterfly}, provide a
useful tool for mapping the long term trends and of the toroidal magnetic field
throughout each solar cycle, aiding numerical simulations. \\

\begin{figure}[t]
  \centering
  \includegraphics[width=0.6\linewidth]{dynamo.png}
  \caption{(a) An initial poloidal magnetic field. Due to the
  \textomega-effect, this field is pulled in the toroidal direction (b),
  eventually creating a toroidal field (c). This toroidal field results in the
  formation of sunspots (c), which in turn generate the large scale poloidal field
  (d) \citep{carroll2006}.}
  \label{fig:dynamo}
\end{figure}


Another process that contributes toward the poloidal magnetic field is the
Babcock-Leighton mechanism. Due to the tilt observed in Joy's-law, some
component of the magnetic dipole in a bipolar-sunspot-pair is in the north-south
direction. As the sunspot pair disperses over time, the surface flows release
some amount of this dipole moment, contributing to the overall poloidal field
\citep{babcock_topology_1961,leighton_transport_1964}. This can be seen in
Figure \ref{fig:mag_butterfly}, near the top of each hemisphere. In principle,
this can in itself lead to a working dynamo, by generating the poloidal field
from the toroidally generated sunspots. \\


Despite many years of research into the solar dynamo, there is much that still
remains unclear, with many questions remaining. In particular, this includes:
\begin{enumerate}
  \item What mechanism is predominantly responsible for converting a toroidal
  field to a poloidal one?

  \item Is the Babcock-Leighton mechanism a crucial part of the dynamo
  mechanism, or just a side-effect of decaying Sunspots?

  \item How constraining is the butterfly diagram? I.e can the structure of the toroidal field be
  directly inferred from the distribution of the Sunspots?

  \item Is the tachocline a crucial part of the dynamo mechanism?

  \item What is the cause of periods such as the Maunder Minimum?
\end{enumerate}
A better understanding of the dynamo will lead to a better understanding of the Sun
and all stars in general. Furthermore, a stronger grasp of the working of the
solar magnetic field, and therefore of sunspots, will be critical in our ability
to predict and prepare for extreme space weather events.








\section{Space weather}
%  Solar flares
%   coronal mass ejection's
%  STEREO spacecraft (3d model of coronal mass ejections)
%     Effects on earth
%  How it is modelled
%  Magnetograms
% Currently they use the flux transport model (ADAPT)
%   full sun magnetograms (currently created by assuming magnetic field doesn't
%   change in the farside,
%i.e. farside is just nearside from half a rotation earlier)
%  how farside magnetograms could help with this



Magnetic activity on the surface of the Sun can at times cause large eruptions
on the solar surface, potentially emitting high-intensity x-rays or ejecting
plasma out into the heliosphere and beyond. While ordinarily harmless, extreme
space weather events can have major consequences, for instance hazardous radiation exposure
to astronauts or significant damage to terrestrial electricity grids. The most extreme
of these space weather events are solar flares and coronal mass ejections. \\


These eruptive events occur in active regions, which as the name
suggests, are magnetically active regions of the Sun that typically consist of
one or more sunspots. Like the bipolar sunspot pairs discussed in Section
\ref{sec:dynamo}, these active regions are generated by the toroidal magnetic
field and rise through the convective layer of the Sun as flux ropes. As these
flux ropes rise, they can twist and kink, often forming knots, leading to the
formation of the more complex active regions \citep{linton_helical_1996}. As they surface,
these newly formed active regions undergo horizontal expansion, known as `pancaking',
releasing some of the accumulated magnetic energy
\citep{toriumi_flare-productive_2019}.\\

Any current in an active region is unable to dissipate efficiently due to
the high conductivity of the plasma. This leads to the build-up in magnetic energy, as the
forces of magnetic pressure, magnetic tension, and gravity cause the flux ropes
in an active region to twist and shear. If the active region is unable to disperse this energy, this
ultimately results in a magnetic reconnection event, where twisted field lines
pointing in opposite directions converge and explosively realign causing a large
release of built-up magnetic energy. This eruption
pushes the flux rope into the higher atmosphere, carrying with it much of the
overhead coronal magnetic field. If the flux rope is ejected successfully, it
forms the magnetic structure of a coronal mass ejection, propelling particles and electromagnetic
radiation outwards into space. This process is known as the `CSHKP' model, named after
the leading researches behind it \citep{carmichael_process_1964,
sturrock_model_1966, hirayama_theoretical_1974, kopp_magnetic_1976}. A visual
depiction of the CSHKP model is shown in Figure \ref{fig:flare_model}. \\


The release of energy in these magnetic reconnection events creates a localised
flash of intense light in the corona, constituting a solar flare
\citep{priest_solar_1984}. X-rays from such a flare can heat the outer
atmosphere of the Earth and increase the drag on satellites at low orbits
\citep{Oliveira2019}, while energetic protons released by a solar flare can
pose a radiation hazard to potential astronauts \citep{Mewaldt2005,
lamarche1996}. This is of particular relevance now, with the recent announcement
of planned missions to land astronauts on the Moon again by 2024, and Mars in
the 2030s \citep{smith_artemis_2020}. \\


Coronal mass ejections pose an even greater hazard to human activity. When a
coronal mass ejection collides with the Earth's magnetic field, it creates a
geomagnetic storm. This deforms the magnetic field and can induce currents in
conductive materials on the Earth in an extreme event. While this has only a
small effect locally, over large scales (such as the long power lines connecting cities), the
cumulative effect can be potentially catastrophic. The Carrington Event in 1859
\citep{carrington_description_1859, hodgson_curious_1859} was the largest such
event ever recorded \citep{cliver_1859_2004}, causing disruptions to North
American and European telegraph systems, with some telegraph operators
experiencing electric shocks (National Research Council,
\citeyear{council_severe_2008}). A smaller geomagnetic storm was observed in
1989 and resulted in communication blackouts due to radio interference, loss of
control from multiple satellites, and mass power outages in Quebec
\citep{odenwald_day_2015}. Due to the large scale electrical grids currently in
place around the world, an event of similar magnitude to the Carrington Event
has the potential to overwhelm electrical grids on a much greater scale. The
National Research Council (\citeyear{council_severe_2008}) estimated that the
recovery of a severe geomagnetic storm would take between 4 and 10 years, and
cost between one and two trillion USD in the first year alone. \\


Prediction and early warning of potentially eruptive active regions is therefore
vital due to the potential hazards. To this end, it is helpful to classify the
different types of active regions. The commonly used Mount Wilson classification
is as follows \citep{martres_etude_1966}:
\begin{itemize}
  \item \(\alpha\): a unipolar sunspot group,
  \item \(\beta\): a bipolar sunspot group with a clear division between the polarities,
  \item \(\beta \gamma\): a complex active region where no single continuous
  line can separate the polarities, and
  \item \(\gamma\): a complex active region with no simple division between the polarities.
\end{itemize}
The qualifier \(\delta\) is used when at least two sunspots of
opposite polarity have umbrae (the centre of the sunspot) separated by less than
two degrees. \\

The probability of eruption increases with the complexity and size of the
active region in the order listed above \citep{giovanelli_relations_1939}. This
relationship is clearly seen in Figure \ref{fig:flare_occurance}. Modern
predictive methods typically use machine learning, based on a set of chosen
parameters, to determine the probability of an active region eruption, and therefore
identify potentially dangerous active regions. For example,
\citet{bobra_solar_2015} used a machine learning algorithm, called support
vector machines, to classify active regions as either flaring or non-flaring. This was
based on magnetograms taken by the Solar Dynamics Observatory's Helioseismic
magnetic imager, using 25 different features of the active region, such as the area and the
total unsigned magnetic flux. \\


However methods such as this have a severe limitation in that any active region
will only be visible for \(\sim 7\) days before directly facing the Earth due to
the rotation of the Sun. To give more advanced warning of potentially dangerous
active regions, a method of imaging the farside magnetic field is needed. \\

\begin{figure}[t]%
  \centering
  \subfigure[]{%
    \includegraphics[height=3in]{flare_occurance.jpg}
    \label{fig:flare_occurance}
  }%
  \qquad
  \subfigure[]{%
    \includegraphics[height=3in]{flare_model.jpg}
    \label{fig:flare_model}
  }%
  \caption{\subref{fig:flare_occurance} The peak flare intensity compared to
  size, for the different active region classes \citep{sammis_dependence_2000}.
  \subref{fig:flare_model} The CSHKP model for a solar flare with a coronal mass
  ejection. The black circles represent regions that can be directly probed with
  hard x-ray observations \citep{Christe2017}}.
\end{figure}







\section{Farside Helioseismic Holography}
\label{sec:FHSM}

% Halo thing (Alina)
% why we can't currently measure the farside or farside magnetic field continuously
%  farside helioseismic holography
% Use of gans to 
%  how this could be used in image-to-image translation to generate farside magnetograms


In Section \ref{sec:HSM} we discussed global helioseismology, the
study of the precise frequencies of the Sun's resonant modes. This can be used
to infer properties about the Sun, such as structure or rotation, as a function
of radial depth and latitude, but gives no details about how these aspects may
change with latitude. Local helioseismology on the other hand instead looks at
spatially compact anomalies in the observed p-modes, caused by some disturbance
\citep{braun_absorption_1988}. Where global helioseismology is analogous to
`hearing' the Sun, local helioseismology is analogous to `seeing' the Sun. Of
particular interest in this thesis is farside helioseismic holography, which
uses the interaction between p-modes and active regions to map the farside of
the Sun. \\

% so far we have used globa helioseismology
% rotation, structure etc

%local helioseismology

% The p-modes used in farside helioseismology propagate downwards from the outer
% convection one, retaining a strong coherence as they travel. If a p-mode
% intercepts an active regions on the Sun's surface, they carry a signature of
% this interaction as they continue to travel. When the encounter the Sun's
% surface, they leave detectable signals, before undergo specular reflection,
% reflecting back into the Sun. This continues for multiple `skips' before they
% either lose coherence or decay (see Figure \ref{fig:skips}).\\

A computational model of the Sun's surface and interior must first be constructed for
any helioseismic holography study. Any acoustic sources or waves in this model
are expressed in terms of an acoustic field, \(\psi\). Disturbances in \(\psi\)
propagate outwards with `bubble'-like wavefronts (see Figure
\ref{fig:egression}). The only part of this model that can be directly observed
is the disturbances that reach the surface, \(S_0\). A record of these surface
disturbances, \(\psi_0\), is then applied to the model. This model is then run
backwards in time, giving a time-reversed acoustic field, \(H_+(\bm{r}, t)\)
(also called the `coherent acoustic egression'), which gives a measure for the
disturbances on a `sampling surface' that travels backward in time with the
acoustic egression through the solar interior (see Figure \ref{fig:egression}).
The acoustic power on this surface is then given by \(\abs*{H_+(\bm{r},
t)}^2\).\\

\begin{figure}[t]%
  \centering
  \subfigure[]{%
    \label{fig:skips}%
    \includegraphics[height=2in]{skips.png}% 
  }%
  \qquad
  \subfigure[]{%
    \includegraphics[height=2in]{egression.jpg}%
    \label{fig:egression}%
  }%
  \caption[]{\subref{fig:skips} Depiction of p-modes propagating through the solar
    interior \citep{Lindsey2017}. \subref{fig:egression} Disturbances on the solar surface (dark blue) can be
    computationally propagated backward in time to a sampling surface. The
    acoustic power is then given by the squared magnitude of this coherent
    acoustic egression.\citep{Lindsey2011} }
\end{figure}


Farside helioseismic holography uses this same concept but also takes advantage of
the diffraction of the p-modes that occurs in the Sun. The diffraction of sound
waves when crossing between two different mediums is given by Snell's law,
\begin{align}
  c_0 \sin \theta = c \sin \theta_0 \,,
\end{align}
where \(c_0\) and \(\theta_0\) are the speed of sound and the angle from the normal
in the initial medium, while \(c\) and \(\theta\) are the speed of sound and
angle from the normal in the medium the wave travels to
respectively\footnote{The normal referenced is the normal to the surface
separating the two mediums,}. Rearranging this in terms of a constant \(K = \sin
\theta_0 / c_0 \), we get
\begin{align}
  \sin \theta = Kc \,.
\end{align}
Approximating the Sun as spherically symmetric, with a sound speed dependent
only on the radial distance from the centre, \(r\), Snell's law transforms to
\begin{align}
  r\sin \theta = Kc \,,
\end{align}
with the initial condition, \((\theta_0, c_0)\), and the new constant,
\begin{align}
  K = \frac{R_\odot \sin \theta_0}{c_0}\,.
\end{align}
The consequence of this is that p-modes travel in the curved paths shown in
Figure \ref{fig:skips}, `skipping' when they reach the surface due to the
specular reflection. Figure \ref{fig:pupil} illustrates how the acoustic waves
can travel to (green arrows) and from (yellow arrows) the `focus' on the
farside. \\

Active regions are strong absorbers of acoustic waves unless the waves approach
in a direction close to the normal of the surface \citep{Braun1989,
lindsey_seismic_2000, braun_surface-focused_2008}, as is the case of those with
skip distances like that of the ones shown in Figure \ref{fig:pupil}. However,
while they do not absorb these approximately normally incident waves, they do
impart a phase shift of a fraction of a radian upon them, which in turn, causes
the echo to reach the nearside a few seconds earlier than it otherwise would
have. This may be due to a physical depression observed in sunspots, called
the Wilson depression \citep{Lindsey_2010}.\\

To detect farside active regions, the p-modes travelling
from the nearside to the focus are compared to the echo that comes back to the
nearside. While the echo is modelled with coherent acoustic egression introduced
above (\(H_+(\bm{r}, t)\)), the waves travelling toward the farside are modelled with
`coherent acoustic ingression', \(H_-(\bm{r}, t)\), which is the time-forward
equivalent. By comparing these two for various pupils, a map of the phase-shifts
and therefore a map of potential farside active regions can be created.
Composite images of the farside seismic map and the corresponding nearside
magnetograms are shown in Figure \ref{fig:phase_map}.\\

\noindent The spatial resolution of this technique is limited by the Abbe
diffraction limit,
\begin{align}
  \Delta s &= 1.22\frac{\lambda_0}{2 \sin{\theta_0}}\,,
\end{align}
where \(\lambda_0\) is the wavelength of the p-mode and \(\theta_0\) is the
`opening angle' of the focus (see Figure 7a). For a double skip,
such as the one shown in Figure \ref{fig:skips}, we have an opening angle of
\(\theta_0 =2.9\degree\), which gives a spatial resolution of \(\Delta s =
10\degree\) of the Sun's surface. For a single skip, we have \(\theta_0
=0.33\degree\), giving the significantly worse spatial resolution of \(\Delta =
87\degree\). \\

\begin{figure}[t]%
  \centering
  \subfigure[]{%
    \label{fig:pupil}%
    \includegraphics[height=3in]{pupil.png}%
  }%
  \qquad
  \subfigure[]{%
    \includegraphics[height=3in]{phase_map.png}%
    \label{fig:phase_map}%
  }%
  \caption[]{
    \subref{fig:pupil} Diagram showing the paths taken by the wavefronts
    travelling to the focus (green) before echoing back from the focus (yellow)
    \citep{Lindsey2011}.\\
    \subref{fig:phase_map} Images showing both a farside seismic map created using
    farside helioseismic holography (yellow) and a nearside magnetogram (blue).
    As the Sun rotates, active regions can be seen moving from the farside to
    nearside. The seismic map is measured by the time perturbation, \(\tau\) caused by
    potential active regions \citep{Lindsey2017}.
    }
\end{figure}


In practice, farside helioseismic holography has been used to produce farside
seismic maps every 12 hours by Stanford's Joint Science Operations
Center\footnote{See \url{jsoc.stanford.edu/data/farside}.}. Both \(H_+\)
and \(H_-\) are calculated over 24 hour (overlapping) periods, using
dopplergrams taken by the Solar Dynamics Observatory's Helioseismic
Magnetic Imager (SDO/HMI). This process takes \(31 \) hours, due to the \(7\) hour
travel time of the acoustic waves. \\


While there is a known correlation between the seismic signatures and the
magnetic flux of an active region \citep{Gonzalez_Hernandez_2007}, a direct relationship
between the phase shift and the magnetic field is unknown, preventing accurate
prediction of potentially dangerous active regions. However, due to the large amounts of data
available, recent machine learning techniques offer the potential of finding
such a relationship.



%
%
\section{Deep learning}
%
%

% Machine learning
% Supervised vs unsupervised
% regression
% structure
% Input
% Fully connected layer
% Convolutional layers
% Learning
% Gradient descent
% Backprop
% Gans
% cGans
% how they could be used to generate farside magnetograms
\label{sec: deep learning}
Machine learning is the process of a computer algorithm improving at some task
through `experience'. In supervised learning (as opposed to unsupervised
learning), this task is to learn some function based on training examples,
each consisting of an input, \(\bm{x'}\), and a corresponding desired output,
\(\bm{y'}\). After training, the resulting function would ideally be able to
take a new input, \(\bm{x}\), and return an appropriate output, \(\bm{y}\). \\

In supervised deep learning this function takes the form of an artificial neural
network, essentially a large composite function:
\begin{align}
  \bm{y} &= NN(\bm{x}, \bm{\theta})\\
  &= L^{[n_L]}(\bm{\theta}^{[n_L]}, L^{[n_L-1]}( \dotsm L^{[1]}(\bm{\theta}^{[1]}, \bm{x}) \dotsm ))\,, 
\end{align}
where each function \(L^{[i]}\) is a `layer' with parameters
\(\bm{\theta}^{[i]}\), and \(n_L\) is the total number of layers. The `deep' in deep
learning refers to the large number of layers between the input and output.
Training is therefore the process of tuning the parameters of the neural
network until it behaves as desired. \\

The past two decades have seen significant improvements in computational
capability and the availability of large data sets. Recent improved deep
learning algorithms have capitalised on this, using their immense flexibility to
tackle problems such as object detection \citep{krizhevsky_imagenet_2017} or
speech recognition \citep{toth_phone_2015}. To understand how these algorithms
work, we must look deeper into the structure of neural networks.

%
% Section: Structure
%


\subsection{Structure}

A neural network consists of many connected `neurons': nodes in the network each
holding some value, originally inspired by biological neurons in the brain
\citep{mcculloch1943}. In `feedforward' neural networks, these neurons are
organised into sequential layers as described above. The data is processed
through the neural network beginning at the input layer, with the outputs of one
layer (the neurons) becoming the inputs to the next, as shown in Figure
\ref{fig:nn} \citep{michelucci2018}. These layers can take a variety of forms.


\subsubsection{Input}
The first layer of a neural network is the input, which has neurons with values
that directly correspond to the data. This is often organised into either a
one-dimensional array or a two-dimensional matrix, with the latter primarily
used when analysing images, where each neuron in the matrix would correspond to
a pixel. Optionally, multiple `channels' can be used, which adds another
dimension to the data. This is typically used if the input is an RGB image, in
which case each pixel would have three values (one for the intensity of each
colour). In this case, three channels would be used, with each channel
representing the intensity of the given colour.\\

Data is often sent into the network in `batches'. This adds another dimension
to the input corresponding to the size of the batch. This has the advantage of
allowing much of the data to be fed through in parallel.

  \begin{figure}[t]%
    \centering
    \subfigure[]{%
      \label{fig:nn}%
      \includegraphics[height=2in]{nn.png}% 
    }%
    \qquad
    \subfigure[]{%
      \includegraphics[height=2in]{perceptatron.png}%
      \label{fig:perceptatron}%
    }%
    \caption[]{ \subref{fig:nn} A feedforward neural network with a single hidden layer.
      \textit{Image courtesy of Wikimedia commons\protect\footnotemark}.
      \subref{fig:perceptatron} A diagram of a single perceptron with inputs,
      \(x_i\), weights, \(w_i\), a bias, \(b\), an activation function,\(f\),
      and output, \(y\). \textit{Image courtesy of Dr Andrew Casey}.}
  \end{figure}

\footnotetext{https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg}



\subsubsection{Fully Connected Layer}
Neurons in a fully connected layer are modelled after the perceptron, originally
conceived by \citet{rosenblatt1958}, and take the form shown in Equation
\ref{eqn:perceptatron}. This consists of a weighted sum over the inputs $x_i$,
with some bias, $b$, and an activation function, $\varphi$, as shown below
\citep{reagen2017}:
\begin{align}
  \label{eqn:perceptatron}
  y = \varphi \left(\sum_{i}{w_i x_i} + b \right)\,.
\end{align}
\\

This can be represented with a graph such as the one in Figure
\ref{fig:perceptatron}. The use of the activation function was originally
inspired by the activation of organic neurons \citep{hodgkin1952}, with the idea
that the artificial neuron is only `activated' when the weighted sum of the
inputs is high enough. Rectified Linear Units (ReLUs) are perhaps the most
widely used activation function in modern neural networks and have been shown to
outperform traditional sigmoid activation functions \citep{glorot2011}. Figure
\ref{fig:activation} shows the sigmoid (left) and ReLU (right) activation
functions.\\


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{sigmoid_v_relu.pdf}
  \caption{Comparison of ReLU (left) and sigmoid (right) activation functions.}
  \label{fig:activation}
\end{figure}

In a fully connected layer all neurons from one layer are connected to all
neurons in the next, hence the name. An example of one such fully connected
layer shown in Figure \ref{fig:fully_connected}. A single layer, \(L\), in a
neural network can be represented by a matrix of weights, \(W^{[L]}\), a
vector of biases \(\bm{b}^{[L]}\), and the activations (value of the neurons) of
that layer \(\bm{x}^{[L]}\). The activations of the next layer,
\(\bm{x}^{[L+1]}\) are then given by
\begin{align}
  \label{eqn:matrix_repr}
  \bm{x}^{[L+1]} &= \varphi \left(W^{[L]}\bm{x}^{[L]} +\bm{b}^{[L]}\right)\,.
\end{align}
\\

\noindent In component form, this is equivalent to
\begin{align}
  \label{eqn:component_repr}
  \bm{x}^{[L+1]}_i
  &= \varphi \left(\sum\limits_j \left(W^{[L]}_{ij}x^{[L]}_j\right) + b^{[L]}_i\right)\,,
\end{align}
where \(\bm{x}^{[1]} = \bm{x}\) would be the input of the network,
\(\bm{x}^{[n_L]} = \bm{y}\) would be the output of the network, and \(n_L\) is
the number of layers. In this case, the weights and biases would be the
parameters of the model, i.e.
\begin{align*}
  \bm{\theta} = \left\{W_{ij}^{[L]}, b_{k}^{[L]} \mid i, j, k, L \in \mathbb{N} \right\}\,.
\end{align*}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\linewidth]{ann.png}
  \caption{The first two (fully connected) layers in a neural network
  represented as a graph. Each circular node represents a neuron, while the
  arrows and weights show the connections between them \citep{michelucci2018}.}
  \label{fig:fully_connected}
\end{figure}

\subsubsection{Convolutional layers}
\label{sec:convolutional}
Convolutional layers in neural networks are typically (although not necessarily)
used when analysing inputs with more than one dimension, such as images or
videos. A neural network consisting of mostly convolutional layers is called a
convolutional neural network. \\


\citet{hubel_receptive_1959} found that neurons in a cat's visual cortex fired
in response to properties of the sensory inputs, such as edges. This was the
inspiration for early convolutional architectures
\citep{fukushima_neocognitron_1980}. Unlike fully connected layers, the neurons
in a convolutional layer are organised into tensors of two or more dimensions.
This is then convolved with a `filter': a tensor that takes up a small portion
of the input. This filter is moved across the input in steps or `strides' of
some size, and the dot product between the filter and the section of input is
computed, which then makes up part of the input for the following layer (see
Figure \ref{fig:convolution}). This gives a measure for the difference between
the filter and the input area, with the idea that the filter will pick up some
feature from the input, for example, an edge in an image. This process typically
reduces the size of inputs between layers, and in this case, is called
downsampling. If the input is first `padded' with extra zeros, the same process
can increase the size of the inputs between layers in which case the process is
called upsampling or deconvolution (see Figure \ref{fig:upsampling}).
Furthermore, multiple filters may be used to create multiple output layers or
equivalently multiple slices of a higher-dimensional output layer. For example,
if two different filters were used on a two dimensional ($100 \times 100$) input,
the output would be a ($100 \times 100 \times 2$) layer with the last dimension
corresponding to each of the two filters. It should be noted that convolutional
layers are equivalent to a fully connected layer with specific weights held at
zero and non-zero weights (which correspond to a filter) are copied such that
the same filter is applied across the image (see again Figure
\ref{fig:convolution}). This mathematical equivalence means that the process of
training a network is the same regardless of whether convolutional or fully
connected layers are used.

\begin{figure}%
  \centering
  \subfigure[]{%
  \label{fig:convolution}%
  \includegraphics[height=2in]{convolution.png}}%
  \qquad
  \subfigure[]{%
  \label{fig:upsampling}%
  \includegraphics[height=2in]{upsampling.png}}%
  \caption[\subref{fig:convolution} Diagram of a convolutional layer \subref{fig:upsampling} Diagram of an upsampling layer]{\subref{fig:convolution} Diagram of a convolutional layer showing the input (blue) and the output (green). A filter is applied to a subset of the input (shaded blue) and the dot product between the entries of the filter and input is returned (shaded green). \\
  \subref{fig:upsampling} Diagram of an upsampling layer, showing the input (blue) with padding (white) and the output (green). Convolution is applied to this in the same way as in \subref{fig:convolution}, however the padding allows for the output to be larger then the original input.\\
  \textit{Both images generated from:} \url{github.com/vdumoulin/conv_arithmetic}.}
  \end{figure}

% Section: Learning
%
\subsection{Learning}
\label{sec:learning}
Typically a neural network learns its parameters, \(\bm{\theta}\), via
supervised learning. The network is first trained using known input/output pairs
\((\bm{x'}, \bm{y'})\), and the model can then be used for inference to estimate
the output (\(\bm{y}\)) of new inputs (\(\bm{x}\)) \citep{reagen2017}. This can
be represented as follows:
\begin{align*}
  (\bm{x'}, \bm{y'}) &\rightarrow NN(\bm{\theta}) && \text{Training} \\
  \bm{x} &\mathrel{\underset{NN}{\rightarrow}} \bm{y} && \text{Inference} \,.
\end{align*}
This training is typically done by using gradient descent, an iterative method
for finding a local minimum in a differentiable function \citep{cauchy_1847}. At
each iteration, beginning at some starting point, the gradient at the current
point is calculated, and a step is taken in the direction of the negative of the
gradient i.e. a step in the direction of the sharpest decline. A depiction of
gradient descent is shown in Figure \ref{fig:gradient_descent} using a contour
map. This has been applied to neural-network-like models since the 1960s
\citep{bryson1962steepest}, where the differentiable function in this case is
the cost function, \(C(\bm{\theta})\), a function in parameter space which gives
a measure for the distance between the outputs of the current model and the
desired outputs. By finding a minimum of this cost function, we effectively find
a point in parameter space with minimal distance between the actual outputs and
the desired outputs, i.e. we have a good model\footnote{This isn't actually
guaranteed, as gradient descent only finds a local minima, and not necessarily
the global minimum.}.\\

A cost function \(C_p\) can be calculated for the individual input/output pairs
\((\bm{x'}, \bm{y'})\). The total cost function, \(C_T\), is then given by the
average of the cost functions for all input/output pairs in the data, as shown
in Equation \ref{eqn:total cost}, where \(n_D\) is the total number of
input/output pairs in the training data.
\begin{align}
  C_T = \frac{1}{n_D} \sum\limits_{p} C_p \label{eqn:total cost}
\end{align}

These cost functions will often take the form of either the quadratic cost, also
known as the mean squared error, (Equation \ref{eqn:quadratic}), or the cross
entropy\footnote{Usually cross entropy is used when the outputs can be
represented as probability functions, i.e. \( 0 < y_i < 1\) and \(\sum y_i =
1\)} (Equation \ref{eqn:cross-entropy}).
\begin{align}
  C_p(\bm{\theta}, \bm{y}, \bm{y'}) &= \frac{1}{2} \sum\limits_{i}
    \left(y'_i - y_i\right)^2 && \text{Quadratic cost} \label{eqn:quadratic}\\
  C_p(\bm{\theta}, \bm{y}, \bm{y'}) &= - \sum\limits_{i}
  y'_i \log{y_i} && \text{Cross-entropy} \label{eqn:cross-entropy}
\end{align}
By minimising the cost function using gradient descent, the neural network
ideally learns the parameters that give a sensible output. To use traditional
gradient descent, the gradient of the total cost function would be calculated at
each step, requiring all the training data to be fed through the network before
taking a single step in parameter space, in addition to increasing the
computational cost of calculating the gradient.\\

To avoid this, stochastic gradient descent is typically used, where an
estimation of the gradient is used instead \citep{Bottou2010}. This estimation of
the gradient is calculated by only looking at a subset of the data (a batch) and
finding the gradient of the average cost function of this batch, i.e. finding
the gradient of:
\begin{align}
  C_B = \frac{1}{n_B} \sum\limits_{B} C_p \label{eqn:average cost}\,,
\end{align}
where \(n_B\) is the number of input/output pairs in the batch. Backpropagation
\citep{rumelhart_learning_1986} is typically used to calculate the gradient of
this average cost function.

\begin{figure}[t]
  \centering
  \includegraphics[width = 0.4\linewidth]{gradient_descent.png}
  \caption{Diagram showing gradient descent on a contour map. \textit{Image
  courtesy of wikimedia commons.}}
  \label{fig:gradient_descent}
\end{figure}


\subsubsection{Backpropagation}
By definition, the gradient of the average cost function is given by
\begin{align}
  \left(\nabla C_B\right)_i &= \partialderivative{C_B}{\theta_{i^{[L]}}} \,.
  \intertext{Using Equation \ref{eqn:average cost}, this gives:}
  \left(\nabla C_B\right)_i &=\frac{1}{n} \sum\limits_p \partialderivative{C_p}{\theta_i^{[L]}}\,.
  \label{eqn:backprop_deriv}
\end{align}

The goal of backpropagation is therefore to calculate the derivative in Equation
\ref{eqn:backprop_deriv} for each parameter \(\theta_i \in \bm{\theta} \)
\citep{Goodfellow-et-al-2016}. \\

From Equations \ref{eqn:quadratic} and \ref{eqn:cross-entropy}, we know that the
cost function is dependent on the output of the network, \(\bm{y}\), and the
desired output, \(\bm{y'}\). While \(\bm{y'}\) is fixed and does not depend on
the parameters of the network, the output \(\bm{y}\) is the activation of the
last layer of neurons (i.e. \(\bm{y} = \bm{x}^{[n_L]}\)), and is itself a
function of the previous layer of neurons, \(\bm{x}^{[n_L - 1]}\), the weights
of that layer, \(W^{[n_L - 1]}\), and the biases of that layer, \(\bm{b}^{[n_L -
1]}\) (see Equation \ref{eqn:matrix_repr}).\\

Using the chain rule, each derivative can then be framed in terms of the
activation of the neuron \(x_i^{[L+1]}\) that depends on the parameter
\(\theta_i^{L}\):
\begin{align}
  \partialderivative{C_p}{\theta_i^{[L]}} &=
  \partialderivative{x_i^{[L+1]}}{\theta_i^{L}}
  \partialderivative{C_p}{x_i^{[L+1]}}
\end{align}
\noindent
While the derivative \(\partial x_i^{[L+1]} / \partial \theta_i^{[L]} \) can
be directly computed using Equation \ref{eqn:matrix_repr}, the derivative
\(\partial C_p / \partial x_i^{[L+1]} \) requires more discussion.\\

If \(x_i^{[L+1]} = x_i^{[n_L]} = y_i\) (i.e. the neuron \(x_i^{[L+1]}\) is an
output neuron in the last layer), then the cost function will be defined
explicitly in terms of the activation of this neuron (Equations
\ref{eqn:quadratic} and \ref{eqn:cross-entropy}), and we can easily calculate
the derivative,
\begin{align}
  \partialderivative{C_p}{x_i^{[n_L]}} = C_p'\left(x_i^{[n_L]}\right)\,.
  \label{eqn:back_prop_1}
\end{align}
\\

However in general, this will not be the case and we must instead use an
iterative process to calculate this derivative. Since the activation of a neuron
in some layer, say \(x_i^{[L+1]}\), is a linear combination of the activation of
the neurons in the previous layer (see Equation \ref{eqn:matrix_repr}), we can
start with Equation \ref{eqn:back_prop_1}, and `propagate' backwards one layer
at a time to find the partial derivative of \(C_p\) with respect to the
activation of each neuron in the previous layer,
\begin{align}
  \partialderivative{C_p}{x_j^{[n_L - 1]}}
  &= \sum\limits_i \partialderivative{x_i^{[n_L]}}{x_j^{[n_L - 1]}}
  \partialderivative{C_p}{x_i^{[n_L]}} \,.
  \label{eqn:back_prop_2}
\end{align}
\\

\noindent We can therefore iterate through the following until we get to the layer
\(k-1\) (or equivalently \(L+1\)):
\begin{align}
  \partialderivative{C_p}{x_j^{[k - 1]}}
  &= \sum\limits_i \partialderivative{x_i^{[k]}}{x_j^{[k - 1]}}
  \partialderivative{C_p}{x_i^{[k]}}\,.
  \label{eqn:back_prop_3}
\end{align}
\\

\noindent Using Equation \ref{eqn:component_repr}, we can explicitly calculate each derivative
\begin{align}
  \partialderivative{x_i^{[k]}}{x_j^{[k - 1]}}
  &= \varphi'W_{ij}^{[k-1]}\,,
\end{align}
allowing us to calculate the gradient \(\nabla C_B\) of the average cost
function for the batch using Equation \ref{eqn:backprop_deriv}. Finally, with
the gradient found, we can now update the parameters of the network by taking a
step in the \(-\nabla C_B\) direction of parameter space. \\

Deep learning techniques based on the fully connected or convolutional neural
networks described above have been very successful at labelling problems such as
speech recognition, \citep{Hinton2012}, or image classification,
\citep{Krizhevsky2012}. However using these techniques to \textit{generate}
data had only experienced limited success before the recent introduction of
generative adversarial networks (GANs).\\

\subsection{Generative Adversarial Networks}
\label{sec:gan}
\citet{Goodfellow2014} introduced GANs as a way of generating
new data that `imitates' data from a given set. Rather than use a single
network, a GAN uses two separate neural networks, a generative network (the
generator) and a discriminative network (the discriminator), that compete
against each other such that the success of one network becomes the loss for the
other. In this process, the generative network learns to generate data similar
to the dataset while the discriminative network learns to distinguish between
samples either taken from the data distribution or generated by the generative
network \citep{Goodfellow2014}. The objective of the generative network is
therefore to increase the error-rate of the discriminative network. Notably, the
generator never actually sees the data it's trying to emulate, only the success
of the discriminator network. The only input to the generator is random noise,
which allows it to generate a new output each time. \\

An analogy of this process given by \citet{Goodfellow2014} is that the generative
network is a counterfeiter, trying to produce a fake currency without being
detected, while the discriminative network is the police, trying to detect the
counterfeit currency.\\

In a traditional GAN, the input of the generator, G, is some noise, \(\bm{z}\),
drawn from some predefined prior (\(\bm{z} \sim  p_z(\bm{z})\)), while
the output, \(G(\bm{z})\) is a mapping to the data distribution. Meanwhile the
input to the discriminator, D, is either samples, \(\bm{x}\), from the data
distribution, \(p_x\), or outputs of the generator, \(G(\bm{z})\). The output of
the discriminator, \(D(\bm{y})\), then represents the probability that the input
came from the data distribution (\( \bm{y} \sim p_{data}\)) and not from the
generator \(\bm{y} = G(\bm{z})\). The discriminator can therefore be trained to
maximise the probability of correctly identifying it's input with the
following cost function (see Section \ref{sec:learning})
\begin{align}
  C_D(D, G, \bm{\theta}_D, \bm{\theta}_G, \bm{x}, \bm{z}) &=
  -\log[D(\bm{x})] - \log[1 - D(G(\bm{z}))]\,,
\end{align}
and so minimising this cost function will maximise the probability of the
discriminator correctly identifying its input.\\

\noindent Conversely, the cost function for the generator is given by
\begin{align}
  C_G(D, G, \bm{\theta}_D, \bm{\theta}_G, \bm{x}, \bm{z})
  &= -C_D(D, G, \bm{\theta}_D, \bm{\theta}_G, \bm{x}, \bm{z})\\
  &= \log[D(\bm{x})] + \log[1 - D(G(\bm{z}))] \,.
  \label{eqn:generator_cost}
\end{align}

Early on in training, Equation \ref{eqn:generator_cost} might not be best suited
as a cost function, since the discriminator will easily be able to reject the
early generator outputs as they will be clearly distinct from the dataset
\citep{Goodfellow2014}. To avoid this, it may be more efficient to instead use
the following cost function at the start of training:
\begin{align}
  C_G(D, G, \bm{\theta}_D, \bm{\theta}_G, \bm{x}, \bm{z})
  &= -\log[D(G(\bm{z}))]\,.
\end{align}

Typically, training is done by alternating between training the generative network
and training the discriminative network until convergence. However GANs that
operate as described above are unable to take in any auxiliary information that
could allow it to condition the output of the generator.

\subsection{Conditional Generative Adversarial Networks}
\label{sec:cgan}
In \citeyear{mirza_conditional_2014}, \citeauthor{mirza_conditional_2014} first
introduced the idea of a conditional generative adversarial network (cGAN) as a
way to condition a GAN on some additional information, \(\bm{c}\), such as a
label or related data. While a traditional GAN is unsupervised and only needs an
input dataset which it learns to emulate, a cGAN is supervised and requires a
labelled dataset, i.e. many \((\bm{x}, \bm{c})\) pairs. This extra information
is fed into both networks allowing it to associate its output with this
additional information. A comparison between a GAN
and a cGAN is shown in Figure \ref{fig:gans}.\\


\begin{figure}[t]
  \centering
  \includegraphics[width = 0.6\linewidth]{gan_cgan.png}
  \caption{Comparison between a GAN (a) and a cGAN (b). In the GAN, noise
  (\(\bm{z}\)) is fed into the generator (\(G\)). The input to the discriminator
  (\(D\)) is then either the `fake' output of the GAN (\(G(\bm{z})\)) or the
  `real' data (\(\bm{x}\)). The discriminator decides if the input it has been
  given is real or fake. In the cGAN, both the generator and discriminator have
  an additional input (\(\bm{c}\)) which `conditions' the data
  \citep{mirza_conditional_2014}. In the case of an image-to-image GAN, this
  conditional data is an image, which is the only input to the generator. }
  \label{fig:gans}
\end{figure}


By conditioning a cGAN on images, this idea can be extended to image-to-image
translation \citep{isola2017image}. In this case, the only input to the generator
is the image \(\bm{c}\), from which the generator must produce an image
\(G(\bm{c})\) that closely matches \(\bm{x}\). For an image-to-image cGAN, the
cost function for the discriminator becomes
\begin{align}
  C_D(D, G, \bm{\theta}_D, \bm{\theta}_G, \bm{x}, \bm{c}) &=
  -\log[D(\bm{x}\mid \bm{c})] - \log[1 - D(G(\bm{c}))]\,,
\end{align}
while the cost function for the generator becomes
\begin{align}
  C_G(D, G, \bm{\theta}_D, \bm{\theta}_G, \bm{x})
  &= \log[D(\bm{x}\mid \bm{c})] + \log[1 - D(G(\bm{c}))]\,.
\end{align}


Image-to-image cGANs have a large potential for disruption in solar physics due
to the large number of images taken by spacecraft and terrestrial observatories
alike. While cGANs have already been used to generate solar magnetograms from EUV images
\citep{Kim2019}, and vice versa \citep{park_generation_2019}, there has been no
research into how they could be used to generate magnetograms from seismic maps.


\section{Producing magnetograms from farside seismic maps.}

%TODO: write this section

\begin{itemize}
  \item This is essential, as the only currently reliable way of imaging the solar
  farside is through farside helioseismic holography, with the generation of
  farside seismic maps.
  \item some research has been done to the correlation of seismic signitures and
  magnetic field (https://iopscience-iop-org.ezproxy.lib.monash.edu.au/article/10.1086/521592)
  \item research into detecting active regions from farside seismic maps ()
\end{itemize}


\begin{itemize}
  \item machine learning requires lots of data
  \item we have lots of freely available data
\end{itemize}




\begin{itemize}
  \item While many Earth based solar observatories exist, space-based
  observatories are able to monitor the Sun without the limitation of
  atmospheric absorption.
  observing the Sun from
  Earth comes with many issues 
  \item The best way to image the Sun is directly from space-based telescopes.
  \item The Solar Dynamics Observatory (SDO) \citep{pesnell_solar_2012} orbits
  the Earth with a suite of instruments including the Atmospheric Imaging
  Assembly (AIA) \citep{lemen_atmospheric_2012} and the Helioseismic and
  Magnetic Imager (HMI) \citep{scherrer_helioseismic_2012}. Pertinent to this
  research, AIA is capable of taking extreme UV images of the full solar disk at
  a wavelength of \SI{304}{\angstrom}, while HMI is capable of taking full-disk
  magnetograms which measure the line-of-sight magnetic field, and full-disk
  dopplergrams which measure the line-of-sight motion of solar surface. As
  detailed in Section \ref{sec:FHSM}, these dopplergrams can be used to generate
  farside seismic maps. However by the nature of orbiting Earth, SDO is unable
  to image the solar farside except indirectly through farside helioseismic
  holography. Only three active solar probes, Parker Solar Probe, Solar Orbiter
  and Solar-Terrestrial Relations Observatory A (STEREO-A) measure the farside
  at any point in their orbit, and only STEREO-A consistently takes full-disk
  solar images. While STEREO-A does not produce magnetograms, the extreme
  ultraviolet imager on board images the Sun at a wavelenth of
  \SI{304}{\angstrom}. 

  Therefore, an indirect
  measurement of the magnetic field must be employed to reliably predict extreme
  space weather events.
  
  \citet{Kim2019} trained an image-to-image cGAN to generate artificial
  magnetograms from Extreme Ultraviolet (EUV) images. This training was done
  using pairs of \SI{304}{\angstrom} EUV images and magnetograms from SDO. This
  trained cGAN was then used to generate artificial farside magnetograms from
  similar STEREO-A EUV images.

  \item however they used a saturation point of 100 Gauss, limiting the ability
  to predict strong magnetic fields, especially considering that active regions
  can have magnetic field strengths of thousands of Gauss

  \item Furthermore, STEREO-A is currently on the nearside coming towards Earth
  and only has a partial view the solar farside (see Figure
  \ref{fig:stereo_pos}).

  \item As such the only consistent observation of the Farside comes from
  farside helioseismic holography.

  \item To generate farside magnetograms from seismic images using an
  image-to-image cGAN, we need to construct a dataset of seismic
  image/magnetogram pairs. However since we have no available farside
  magnetograms, the creation of this dataset presents it's own issue. To
  overcome this issue, we first train an image-to-image cGAN to generate
  magnetograms from \SI{304}{\angstrom} solar EUV images.

\end{itemize}



%
% Project outline from lit review
%

To train a cGAN to generate magnetograms from seismic maps, we require a
training set consisting of seismic maps and the corresponding magnetograms.
While the farside seismic maps are readily available\footnote{See
  \url{jsoc.stanford.edu/data/farside}.}, corresponding magnetograms are
not. However, not all hope is lost. The STEREO-A spacecraft is in a heliocentric
orbit that traverses the Sun relative to the Earth, allowing it to observe the
farside during some points of the orbit. While STEREO-A does not capture
magnetograms of the Sun, it does take EUV images which can be used to create
magnetograms by using a cGAN \citep{Kim2019}.\\

A small complication is that no data is available from when STEREO-A was
directly opposite the Earth (between March and July 2015), due to the
interference from the Sun. This limits the ability to get magnetograms that
exactly coincide with the farside seismic maps. To compensate for this, we can
match farside seismic maps with images taken by STEREO-A at an earlier (later)
time when it is behind (ahead of) the farside, such that the same `face' of the
Sun is imaged due to it's rotation. After generating the magnetograms from the
STEREO-A EUV images, we can create a dataset of seismic maps with the
corresponding magnetogram (albeit with some time difference). This process is
summarised in Figure \ref{fig:Project_summary}.\\



\begin{figure}
  \centering
  \begin{tikzpicture}[node distance=5.5cm]
    % \draw[help lines] (0,-5) grid (10,10);
    \node (D) [cool, above=1.5cm] {SDO Dopplergram}; \node (P) [cool, right
      of=D] {Farside \quad \quad \quad \qquad  seismic map}; \node (E) [cool]
    {STEREO EUV image}; \node (M) [cool, right of=E, below=0.5cm] {STEREO
      Magnetogram}; \node (F) [cool, right of=M, above=1cm] {Farside\\
      Magnetogram}; \node (S) [cool, below=1.5cm] {SDO Magnetogram};

    \coordinate (FF) at ([xshift=-1.5cm]F.west); % we collect the edges in
    front of Q
    \coordinate (MM) at ([xshift=-1.5cm]M.west); % we collect the edges in
    front of Q

    \draw [arrow] (D) -- node[anchor=south, text width=2cm] {Helioseismic
      Holography} (P); \draw [arrow] (MM) -- node[anchor=south] {cGAN} (M);
    \draw [line] (P) -|  (FF); \draw [line] (M) -|  (FF); \draw [line] (S) -|
    (MM); \draw [line] (E) -|  (MM); \draw [arrow] (FF) -- node[anchor=south]
    {cGAN} (F);

  \end{tikzpicture}
  \caption[Project Pipeline]{Summary of the project. A cGAN is trained on SDO
    data to be able to generate EUV images. This is applied to STEREO EUV images
    to generate STEREO magnetograms. These are used in conjunction with farside
    seismic maps to train a new cGAN to generate magnetograms from seismic maps,
    allowing constant surveillance of the farside magnetic field.}
  \label{fig:Project_summary}
\end{figure}



%
%
%
%
\chapter{Data Preparation}
\label{chap:data}
%
%
%
%

To generate farside magnetograms from farside seismic maps, we first create two distinct
data sets
\begin{enumerate}
  \item a nearside dataset consisting of EUV and magnetogram image pairs, and
  \item a farside dataset consisting of seismic map and EUV image pairs.
\end{enumerate}
As detailed in Chapter \ref{chap:training}, the farside EUV images will be used
to generate magnetograms, which can then be used to train an image-to-image cGAN
to generate magnetograms from farside seismic maps. Figure
\ref{fig:simple_diagram} shows a summary of the project pipeline. \\
\begin{figure}
  \centering
  \begin{tikzpicture}[node distance=4cm]

      % \draw[help lines] (0,0) grid (10cm,10cm);

      \coordinate (T) at (5,4);

      \node (SGAN) [cool, right of=T, left=0.0cm] {Seismic\\ GAN};
  
      \node (SM) [cool, left of=T, above=1cm, right=0.5cm] {Farside\\ Seismic Map};
      \node (STEM) [cool, left of=T, below=1cm, right=0.5cm] {STEREO Magnetogram};

      \node (SDO) [cool, left of=SM] {SDO: EUV and \\ Magnetogram};
      \node (UVGAN) [cool, left of=STEM] {UV\\ GAN};
      
      % \node (E) [cool] {STEREO EUV image};
      
      \node (FM) [cool, right of=SGAN] {Farside\\ Magnetogram};

      \coordinate (I1) at ([xshift=0.3cm]UVGAN.east);

      \node (SUV) [cool, below of=I1, above=1.5cm] {STEREO-A:\\ EUV};
      \draw [->] (SUV) -- node[anchor=west] {Inference} (I1);

      \coordinate (I2) at ([xshift=0.3cm]SGAN.east);

      \node (SM2) [cool, below of=I2, above=0.5cm] {Seismic\\ Map};
      \draw [->] (SM2) -- node[anchor=west] {Inference} (I2);

      \draw [line] (STEM) -| (T);
      \draw [line] (SM) -| (T);
      \draw [arrow] (T) -> node[anchor=east, xshift=-0.3cm] {Train}(SGAN);

      \draw [arrow] (SDO) -> node[anchor=west] {Train}(UVGAN);
      \draw [arrow] (UVGAN) -> (STEM);
      \draw [arrow] (SGAN) -> (FM);

      % \coordinate (FF) at ([xshift=-1.5cm]F.west); % we collect the edges in front of Q
      % \coordinate (MM) at ([xshift=-1.5cm]M.west); % we collect the edges in front of Q
      
      % \draw [arrow] (D) -- node[anchor=south, text width=2cm] {Helioseismic Holography} (P);
      % \draw [arrow] (MM) -- node[anchor=south] {cGAN} (M);
      % \draw [line] (P) -|  (FF);
      % \draw [line] (M) -|  (FF);
      % \draw [line] (S) -|  (MM);
      % % \draw [line] (E) -|  (MM);
      % \draw [arrow] (FF) -- node[anchor=south] {cGAN} (F);

  \end{tikzpicture}
  \caption{A simplified diagram of the project pipeline. Nearside SDO
  EUV/magnetogram image pairs are used to train the ``UV GAN'', which is then
  used to generate farside STEREO magnetograms. Farside seismic map/STEREO
  magnetogram pairs are then used to train the ``Seismic GAN'' which can then be
  used to generate farside magnetograms from nearside seismic maps without the
  need for STEREO data.}
  \label{fig:simple_diagram}
\end{figure}
To maximise effectiveness of each cGAN, we need to make the images consistent
across each dataset such that the only differentiation between images is the
change in solar activity. Furthermore, in each image-to-image translation we
need to ensure that the active regions are located in the same position in both the
input and output images. We must therefore account for the following effects
\begin{enumerate}[i]
  \item changes in time of image capture, 
  \item changes in location of image capture,
  \item the solar cycle in which the image was taken,
  \item position of the sun in images,
  \item orientation of the Sun in images,
  \item projection used in image,
  \item corrupted images or images with data artifacts,
  \item instrument degradation over time,
  \item instrument saturation, and
  \item amplitude of pixel values between image data-sets.
\end{enumerate}
In this chapter we detail how we obtain our data and prepare it for training,
accounting for above effects.

% \begin{itemize}
  
%   \item Nearside vs Farside (i.e. why we need it)
%   \item Data sources were:
%         \begin{itemize}
%           \item STEREO EUV images
%           \item SDO EUV images
%           \item SDO magnetograms
%           \item farside helioseismic holography maps
%         \end{itemize}
%   \item magnetograms measure the radial magnetic field, using filtegrams
%   \item % https://link.springer.com/content/pdf/10.1007/s11207-011-9834-2.pdf
%   \item figure showing positions and trajectory of stereo, sun earth, SDO and
%         imaginary satellite for acoustic maps
%   \item figure (maybe the same one) showing plot of stereo position in
%   \item position data from http://www.srl.caltech.edu/STEREO/docs/position.html
%   \item This is in Heliocentric Earth equatorial (HEEQ): This system has its Z
%         axis parallel to the Sun's rotation axis (positive to the North) and its X
%         axis towards the intersection of the solar equator and the solar central
%         meridian as seen from the Earth. This system is sometimes known as
%         heliocentric solar (HS)
% \end{itemize}



\section{Data Collection}
\label{sec:data_collection}
Our research required the following data sources:
\begin{enumerate}
  \item SDO EUV images,
  \item SDO magnetograms,
  \item STEREO-A EUV images, and
  \item helioseismic holography maps.
\end{enumerate}
EUV images with a wavelength of \SI{304}{\angstrom} are produced by both the SDO
and STEREO EUV telescopes and were therefore choosen for this project. As explained in
Section \ref{sec:Sun}, light at \SI{304}{\angstrom} is emitted by the
chromosphere - the atmospheric layer above the photoshpere. \\

As SDO is orbiting the Earth, both the SDO EUV images and magnetograms were
taken of the nearside. STEREO-A is instead in a heliocentric orbit, with an
orbital period of 346 days. As such, it has rotated about the Sun relative to
Earth taking some images of the farside over the course of it's 14 year life.
Figure \ref{fig:stereo_pos} shows the location of STEREO-A over the course of
the mission. Finally, the seismic maps are generated from SDO dopplergrams, and
image the farside of the Sun as detailed in Section \ref{sec:FHSM}. \\

Of particular importance when collecting the data is the time of image capture, the
location of image capture, and the solar cycle in which the image was taken in.
Here we detail what decisions were made in regards to these factors.

% Changes in time of image capture
% Changes in location of image capture
% The solar cycle in which the image was taken


\subsection{Nearside Data}
As all the nearside data comes from SDO the position of the telescope does not
change between data types. Furthermore, SDO captures EUV images and magnetograms
with a cadence of 12 and 45 seconds respectively, allowing us to compare these
with very little time difference. The images were provided by the Joint Science
Operation Centre\footnote{See \url{http://jsoc.stanford.edu}} and were
collected for every 12 hours between April 2010, when the first SDO data became
available, and December 2019 - the end of Solar cycle 24. Since all data was
taken during this solar cycle, we did not have to take into acount the flipping
of the global magnetic field which occurs between solar cycles (see Section
\ref{sec:dynamo}). Due to a combination of missing or poor quality images (see
Section \ref{sec:Data prep}) this process resulted in a total of 4247 nearside
EUV/magnetogram pairs. While the nearside data was taken from only a single data
source (SDO), making the data collection relatively simple, this is not the case
for the farside data.

\subsection{Farside Data}
Unlike the nearside data, the farside data comes from two seperate sources.
STEREO-A provides the farside EUV images, while the SDO provides the
dopplergrams that are used to generate the farside seismic maps. Complicating
matters further, for the majority of it's mission STEREO-A is not directly
facing the farside and only has a partial view. Futhermore, STEREO-A experienced
reduced telemetry rates between August 2014 and January 2016, with complete
instrument shut off between March and July 2015 due to STEREO-A's superior
solar conjunction \citep{ossing_stereo_2017}. Figure \ref{fig:stereo_pos} shows
the trejectory of STEREO-A relative to earth with the points of reduced or no
telemetry indicated. \\

To overcome this limitation, we can leverage the rotation of the Sun and compare
farside seismic maps to STEREO-A images with a time delay, such that both images
capture the same `face' of the Sun. For example, if STEREO imaged the Sun while
\SI[]{45}[]{\degree} from the solar farside, after approximately 3 days the Sun
would have rotated such that the same face of the Sun would now be on the
farside, and could be imaged by farside helioseismic holography. By using such a
method, we can effectively compare farside seismic maps with not-quite-farside
STEREO EUV images. This method isn't perfect however, and has two obvious drawbacks
\begin{enumerate}
  \item the differential rotation of the Sun means that active regions won't
  necessarily be in the same position after a time delay, and
  \item active regions are constantly changing, for example new active regions
  may emerge on the surface over the course hours or days, while the decay of
  sunspots may last from days to weeks
  \citep{van_driel-gesztelyi_evolution_2015}.
\end{enumerate}
These limitations will be further discussed in Chapter \ref{chap:discussion}. \\

In order to implement this correction, we must first determine the rotational
rate of the Sun. As can be seen from Figure \ref{fig:butterfly diagram}, the
majority of the active regions in solar cycle 24 are at latitudes between $\pm
\SI{30}{\degree}$. Futhermore the rotation of the Sun is roughly homogoneous at
these latitudes, varying between \SI{425}{nHz} and \SI{450}{nHz} (see Figure
\ref{fig:solar_rotation}). We chose to estimate this rotation rate based on the
Carrington rotational period of \SI{27.2753}{days}. This corresponds to the
average synodic rotational period of sunspots, or equivalently, the synodic
solar rotation at a latitude of approximately \(\SI[]{26}[]{\degree}\)
\citep{carrington_observations_1863}. It should be noted that the synodic
rotation is measured relative to Earth (and therefore the solar farside). An
appropriate choice of coordinates for the correction calculations is therefore
the heliocentric Earth equatorial coordinate system. In these coordinates, the
$z$-axis is aligned with the axis of solar rotation, while the $x$-axis points
from the centre of the sun the earth (see again Figure \ref{fig:stereo_pos}).
The time delay between a STEREO-A image and the farside is then given by
\begin{align}
  \Delta t(t_s) = \left(\frac{\theta(t_s)}{2\pi}\right)T \, ,\\
  \intertext{with}
  \theta(t_s) = \arctan{\left(\frac{y(t_s)}{x(t_s)}\right)} \, ,
\end{align}
where $T$ is the aforementioned Carrignton rotational period, $\theta(t_s)$ is
the angle between STEREO-A and the solar farside, and $(x(t_s), y(t_s))$ is the
position of STEREO-A at time $t_s$ in heliocentric Earth equatorial coordinates.
It should be noted that $\theta(t_s)$ and therefore $\Delta_t$ are negative
while STEREO-A is `behind' the solar rotation, and positive while STEREO-A is
`ahead' of the solar rotation. We can therefore calculate the equivalent farside
time ($t_f$) for a given $t_s$ as follows
\begin{align}
  t_f = t_s - \Delta t(t_s) \, .
\end{align}
This was used to calculate the `farside equivalent' time at each point in
STEREO-A's orbit, using STEREO-A position data provided by the Space Radiation
Lab at California Institute of Technology\footnote{See
\url{http://www.srl.caltech.edu/STEREO}.}. \\

The Joint Science Operation Centre has been producing farside seismic maps with
a cadence of 12 hours since April 2010. These images were downloaded from
Stanford's Joint Science Operations Center\footnote{See
\url{http://jsoc.stanford.edu/data/farside/Phase_Maps}.}. The equivalent time
for STEREO-A was found for each image and the \SI{304}{\angstrom} EUV image that
best matched this time was found. If the image time disagreed with the ideal
time by more than 2 hours the image was discarded. As STEREO-A produces
\SI{304}{\angstrom} EUV images with a cadence of \SI{10}{minutes} this only
effected images produced during periods of reduced telemetry. The remaining
images were downloaded from the Virtual Solar Observatory\footnote{See
\url{virtualsolar.org}.}. \\

However before using these images we need to ensure that the Sun is represented
consistently across each dataset such that any solar features appear in the
same location.

\begin{figure}[ht]
  \centering
  \includegraphics[width = 0.7\linewidth]{STEREO_pos.pdf}
  \caption[STEREO A Trajectory]{Trajectory of STEREO A between October 2006 and January 2021 in the
  Heliocentric Earth Equatorial coordinate system. In these coordinates, the Sun
  is at the origin with the Earth fixed on X axis. Each `bump' in STEREO's
  Trajectory correspond to a year on Earth. \textit{Image generated using
  data provided by the Space Radiation Lab at California Institute of Technology.}}
  \label{fig:stereo_pos}
\end{figure}


\section{Image Projections}
\label{sec:proj}
% Position of the sun in images
% Orientation of the Sun in images
% Projection used in image
To ensure consistency between image datasets we need to take into account the
position and orientation of the Sun as well as how the Sun is represented on
each image. As there are many ways to project three dimensional data onta a two
dimensional image consistent represenation of the Sun is not gaurenteed.
Therefore to effectively compare different images of the Sun we must take into
account the projection used to construct the image.

\subsection{Nearside Data}
Fortunately,  both SDO EUV images and magnetograms use the same projection. For
these images, each pixel directly corresponds to a pixel on the camera sensor,
which in the case of SDO and STEREO-A images is a charge-coupled device or
CCD\citep{kaiser_stereo_2008,lemen_atmospheric_2012}. As the CCD is flat plane,
the resultant image is a projection of the Sun onto the parallel tangent plane of the
celestial sphere.
% gnomonic projection
For both SDO and STEREO-A, the angle subtended by the solar disk is
approximately \SI[]{0.5}[]{\degree} and so we can instead approximate the image
to be projected against the celestial sphere itself. This is a very good
approximation, and at a distance of 1 AU (approximately the orbital radius of
SDO and STEREO-A) the angles describing the Sun on the tangent plane match the
angles on the celestial sphere to at least five significant figures
\citep{thompson_w_t_coordinate_2006}. This projection is known as a
helioprojective-cartesian projection, and measures positions in terms of the
longitude \(\theta_x\) and latitude \(\theta_y\) of the celestial sphere.\\

SDO uses two distinct instruments to create the EUV and magnetogram data
(Atmospheric Imaging Assembly \citep{lemen_atmospheric_2012} and the
Helioseismic and Magnetic Imager \citep{scherrer_helioseismic_2012}
respectively) which have different orientations. The helioprojective latitude
and longitude were found for each pixel using image metadata which was then used
to rotate the images such that the uppermost section of the image corresponded to
the northenmost solar disk region, thus aligning the images consistently.\\

However the distance to the Sun changes throughout SDO's orbit due to the
eccentricty of Earth, changing the relative size of the solar disk. To remove
this discrepancy, while also removing unnecessary pixels, the images were
cropped to the radius of the sun, again using information extracted from the
image metadata. This process is less straight forward for the farside data.

% not sure if I want to keep it like this or change it to below:
%   \item Fortunately both EUV and magnetogram images are taken by SDO at very similar
%   times, and both sets of images are projected into helioprojective-cartesian
%   coordinates.
%   \item In this coordinate system, observations are projected against the
%   celestial sphere, and positions are measured in the longitude \(\theta_x\) or
%   latitude \(\theta_y\) of the celestial sphere, with the centre of the disk
%   (i.e. the point of the Sun closest to the observer) at the origin.
%   \item Note: this is not exactly correct, as the camera sensor (in this case a
%   CCD detector) is a flat plane. As such, we are actually projecting on to a
%   tangent plane of the celestial sphere (gnomonic projection). However since the
%   angle subtended by the solar disk is \(\approx \SI[]{0.3}[]{\degree} \), this
%   can be accurately approximated as a helioprojective projection. In fact, at a
%   distance of 1 AU (approximately the orbital radius of SDO and STEREO), the
%   angles on the tangent plane are the same as the angles on the celestial sphere
%   to at least five significant figures \cite{thompson_w_t_coordinate_2006}
%   \item The helioprojective latitude and longitude of each pixel on the EUV and
%   magnetogram images could be found using the metadata from each image.
%   \item The instruments used to create these images are rotated relative to each
%   other.

\subsection{Farside Data}
While the STEREO-A EUV data uses the same helioprojective-cartesian projection
as the nearside SDO data, this is not the case for the farside seismic maps. The
seismic maps instead us a Carrignton heliographic projection, where positions
are measured in solar latitude (\(\Theta\)) and Carrington longitude
(\(\Phi_c\)). This coordinate system rotates with the Sun such that the prime
meridian of these coordinates faces Earth approximately every 27
days\footnote{To be precise, the prime meridian rotates such that it aligns with
the solar central meridian (according to an observer on Earth) once every
Carrignton rotation (27.2753 days).}. To directly compare STEREO-A EUV images
with farside seismic maps we must therefore re-project the seismic maps into
helioprojective-cartesian coordinates. \\

To transform the seismic maps, we need to find the points in the original
heliographic projection that correspond to each pixel in the final
helioprojective image. In general, these points will not dirrectly correspond to
the centre of a pixel and so we must first apply an image interpolation method
to construct a continuous version of the original heliographic image. To find
these points we use an intermediate transformation to heliocentric-cartesian
coordinates, i.e. 
\begin{align*}
  \text{Helioprojective-cartesian} \rightarrow \text{Heliocentric-cartesian} \rightarrow \text{Carrington Heliographic} \, .
\end{align*}
Heliocentric-cartesian coordinates give the true spatial position of an object
($x$, $y$, $z$) with the origin at the centre of the Sun, the z-axis pointed
toward the observer, and the y-axis in the plane containing the z-axis and the
rotational axis of the Sun. The x-axis is oriented such that the all three axes
create an orthogonal right-handed coordinate system. \\


To convert helioprojective-cartesian coordinates into heliocentric-cartesian
coordinates, we use the following transformation, provided by
\citet{thompson_w_t_coordinate_2006}:
\begin{align}
  x &= d \cos \theta_y \sin \theta_x \, , \\
  y &= d \sin \theta_y \, and \\
  z &= D_\odot - \cos \theta_y \cos \theta_x \, .
  \label{eqn:heliop_to_helioc}
\end{align}
Where \(d\) is the distance is the distance between the observer and the point
being observed, and \(D_\odot\) is the distance between the observer and the centre of
the Sun. After some trigonometry, it can be shown that if the point being
observed is on the surface of the Sun, then
\begin{align}
  d &= D_0 \cos\theta - \sqrt{D_\odot^2 \left( \cos^2\theta - 1 \right) + R_\odot } \, , \\
  \intertext{where}
  \theta &= \cos^{-1}\left(\cos\theta_y \cos\theta_x \right) \, .
\end{align}

Similarly, we can convert from heliocentric-cartesian coordinates to Carrignton heliographic
coordinates as follows:
\begin{align}
  \Theta &= \sin^{-1}\left( \frac{y \cos B_0 + z \sin B_0}{r}\right) \, , \\
  \Phi_c &= \arg (z \cos B_0 - y \sin B_0, x) + \Phi_{0} \,
  \intertext{where}
  r &= \sqrt{x^2 + y^2 + z^2} \, ,
\end{align}
and \(B_0\) and \(\Phi_{0}\) are the Carrington heliographic latitude and
longitude of the Observer. \\


Since we are directly comparing the seismic maps to STEREO-A EUV images, we
choose values \(B_0\), \(\Phi_0\) and \(D_\odot\) according an observer at STEREO-A,
with the longitude adjusted to be opposite Earth (i.e. the centre of the
farside). These exact values were obtained from the STEREO-A image metadata.
The seismic maps were re-projected using this transformation with a bi-linear
interpolation. Figure \ref{fig:projection} shows a seismic map before and after
this transformation. \\
%TODO: overlay stereo and seismic maps or at least show stereo as well. Also
%maybe inbetween image

Now with consistant projection, the STEREO-A EUV images were prepared in the
same manner as for the nearside, by first rotating the images such that the
northenmost region of the solar disk was at the uppermost point in the image,
then cropping the images to the radius of the Sun. This step was not required
for the re-projected seismic maps as they were already correctly aligned as a
by-product of the transformation. With all images correctly aligned, it
is now necessary to take into account effects caused by each imaging instrument.

\begin{figure}[t]%
  \centering
  \subfigure[]{%
    \label{fig:heliographic}
    \includegraphics[height=1.5in]{heliographic.png}%
  }%
  \qquad
  \subfigure[]{%
    \label{fig:helioprojective}%
    \includegraphics[height=1.5in]{helioprojective.png}% 
  }%

  \caption[]{\subref{fig:heliographic} shows an original seismic map with a
  heliographic-cartesian projection. \subref{fig:helioprojective} shows the same
  image after projection into helioprojective-cartesian coordinates.}
  \label{fig:projection}
\end{figure}



\section{Data pre-processing}
% Amplitude of pixel values between image data-sets
% Instrument degradation over time
% Instrument clipping
% Corrupted images or images with data artifacts

\label{sec:Data prep}
Before using the images we need to first take into account various factors and
inconsistancies that arise during the imaging process. Of particular importance
is the EUV data which comes from two data sources, SDO and STEREO-A. To use
data from both sources interchangably, we must make the images consistant between
these datasets.

\subsection{Extreme Ultraviolet Data}
\label{sec:UV_prep}
As mentioned in Section \ref{sec:proj}, STEREO-A and SDO both use a CCD to image
the Sun at a wavelength of \(\SI[]{304}[]{\angstrom}\). Each pixel in the CCD
converts the incoming photons into electric charge, which is subsequently
measured. The value of each pixel in units of digital number (DN), is given by
the integral 
\begin{align}
  \label{eqn:pixel value}
  p(\bm{x}) = \int_0^\infty \eta(\lambda) \int_{\text{pixel } \bm{x}} I(\lambda, \bm{\theta}) d\bm{\theta} d\lambda\, ,
\end{align}
were $I(\lambda, \bm{\theta})$ is the spectral radiance of pixel $\bm{x}$ at point
$\bm{\theta}$ and wavelength $\lambda$, and $\eta(\lambda)$ represents the efficiency
of the \SI{304}{\angstrom} channel in the telescope, measured in units of
DN per unit flux \citep{boerner_initial_2012}. Informally,
$\eta(\lambda)$ is equivalent to the ratio of the signal strength (in DN) of the
CCD to the total electromagnetic flux at wavelength $\lambda$ incident on pixel
$\bm{x}$. As we are attempting to measure the flux at \SI{304}{\angstrom},
$\eta$ would ideally be large for wavelengths close to
\SI{304}{\angstrom}, vanishing as we move away from this wavelength.
Approximating $\eta$ to be zero for $\lambda \neq \SI{304}{\angstrom}$, Equation
\ref{eqn:pixel value} reduces to
\begin{align}
  p(\bm{x}) &= \eta(\SI{304}) \int_{\text{pixel } \bm{x}} I(304, \bm{\theta}) d\bm{\theta} \\
  \intertext{and so,}
  p(\bm{x}) &\propto \Phi(\bm{x}, 304) \, , \label{eqn:pixel propto flux}
  \intertext{where}
  \Phi(\bm{x}, \lambda)& = \int_{\text{pixel } \bm{x}} I(\lambda, \bm{\theta}) d\bm{\theta}
\end{align}
is the electromagnetic flux at wavelength $\lambda$ incident on pixel $\bm{x}$.
And so we find our pixel values are linearly proportional to the
EUV flux to a reasonable approximation.\\


Each raw image is processed to remove data artifacts caused by the the imaging,
and the resulting image is then made available for use. This image processing is
not perfect however, and we still have make some corrections ourselves before we
are ready to use the images.

From the 4313 SDO EUV images used between 2010 and 2020, the pixel values for the
AIA data range from \(\SI[]{-166}[]{DN}\) to \(\SI[]{16383}[]{DN}\). To get a
handle on this data, a range of the pixel value percentiles were calculated for
each image. Figure \ref{fig:aia_percentiles} shows these percentiles plotted as
a function of time. Corrupted or otherwise poor quality images could then be
identified due to the large irregularity in the percentiles of those images, as
can be clearly seen in Figure \ref{fig:aia_outliers}. After reviewing the
offending images, a simple threshold was used to remove the outliers.

Also apparent from Figure \ref{fig:aia_outliers} is decreasing exposure of the
images between 2010 and 2020. This is consistant with the degredation of the
SDO's \SI{304}{\angstrom} EUV channel found by \citet{boerner_photometric_2014}.
Figure \ref{fig:aia_degradation} shows a comparison in the exposures of
images taken in 2011, 2015 and 2019 respectively, in which the reduced exposure
can be clearly seen. To account for this, the pixel
values of each image were given a weighting factor depending on the time the
image was taken, i.e.
\begin{align}
  p_f = w(t) p_i \, ,
\end{align}
where $p_i$ is the initial pixel value, p_f is the final pixel value, and $w(t)$
is the weighting factor at time $t$. The weighting factor was choosen to be the 
reciprocal of a 50-point rolling average of the 75th
percentile at time $t$, i.e.
\begin{align}
  w(t) = \frac{1}{\sum\limits_{i=-25}^{25}P_{75}(t + i \Delta t)} \, ,
\end{align}
where $P_{75}(t)$ is the 75th percentile pixel value of the image taken at time
$t$, and $\Delta t$ is the time between images, in our case 12 hours. The 75th
percentile was picked as it had the lowest 50-point relative variance of the
percentiles calculated. This indicated that the 75th percentile was more
indicative of the background Sun as opposed to individual active regions, and
would therefore better capture the degredation of the instrument over time. It
should be noted however that the 75th percentile was still effected by the solar
activity, and some of this information was inadvertently removed in this
process. This will be further discussed in Chapter
\ref{chap:results_and_analysis}. The percentiles of the data after applying this
weighting are shown in Figure \ref{fig:aia_no_degradation}. \\

To ensure consistency between the two EUV datasets, we need to normalise and
correct the STEREO-A data in the same method as the SDO data. Figure
\ref{fig:stereo_percentiles} shows the initial percentiles of pixel values for
the STEREO-A data, with the times of reduced and no telemetry clearly visible
from the gaps in the dataset. Once again, poor quality images could be
identified by the large deviations in the percentile values, and were removed
using a simple cutoff criteria (see Figure \ref{fig:stereo_outliers}).\\

It was found that approximately $3 \%$ of the pixels in the SDO data had a value
below 0, while the same percentage of pixels from STEREO-A images had a value
below $\SI[]{725}[]{DN}$. Accordingly, the STEREO-A pixels were decreased by
$\SI[]{725}[]{DN}$ before dividing by the rolling average of the 75th
percentile. Figure \ref{fig:stereo_no_degradation} shows the pixel value
percentiles of the STEREO-A images after this process. At this stage in the
normalisation process both SDO and STEREO-A datasets had approximately $3 \%$ of
the data had a value below \SI{0}{DN}, and $75 \%$ of the data with a value
below \SI{1}{DN}. As the pixel values are (approximately) linearly proportional
to the EUV flux (see Equation \ref{eqn:pixel propto flux}), these two points are
enough to constrain the two datasets such that a given pixel value will
correspond to the same level of EUV flux for images in either dataset taken
at roughly the same time. \\
% Assuming that the 75th percentile is the same (flux) at that time.

One last discrepeny between the two datasets is the saturation points at which
the CCD cannot record any value above. While both instruments initialy have a
saturation point at $\approx \SI[]{2e4}[]{DN}$ (see Figures
\ref{fig:aia_percentiles} and \ref{fig:stereo_data_prep}), this has been
distorted during our image processing, as can be seen in Figures
\ref{fig:aia_no_degradation} and \ref{fig:stereo_no_degradation}. To make this
consistant between the STEREO-A and SDO images, we introduce our own artificial
saturation point. To choose this upper bound, we found the minimum of a 50
rolling average of the 100th percentile of pixel value for the STEREO-A data,
which was \SI[]{32}[]{DN}. The upper-bound can be seen in Figure
\ref{fig:stereo_no_degradation}. To avoid lossing information about regions of
intense solar activity, it was important to make this upperbound as high as
possible while keeping the two datasets consistant. This constraint did not
apply to a lower bound, which was chosen to have a value of \SI{0}{DN}. Finally
both datasets were normalised by dividing by the upperbound such that all pixels
had a value between 0 and 1. Figure \ref{fig:sdo_stereo_comparison} shows a
comparison between the STEREO-A and SDO datasets after normalisation. As can be
seen, the datasets seem to be largely consistent with each other. Furthermore,
not all information about the long-term solar activity has been lost through
this normalisation process, as can be seen by the peak in the middle of the
solar cycle around 2014. With the EUV data normalised, it is now time to turn to
the Magnetogram and seismic data. %TODO: fix this sentence



\begin{figure}[t]%
  \centering
  \subfigure[]{%
    \label{fig:aia_2011}%
    \includegraphics[height=1.5in]{AIA_2011.01.01_00:00:00.png}% 
  }%
  \qquad
  \subfigure[]{%
    \includegraphics[height=1.5in]{AIA_2015.01.01_00:00:00.png}%
    \label{fig:aia_2015}%
  }%
  \qquad
  \subfigure[]{%
    \includegraphics[height=1.5in]{AIA_2019.01.01_00:00:00.png}%
    \label{fig:aia_2019}%
  }%
  \caption[]{Images taken by SDO AIA $\SI[]{304}[]{\angstrom}$ on the first of January in 2011
    \subref{fig:aia_2011}, 2015 \subref{fig:aia_2015} and 2019
    \subref{fig:aia_2019}. Due to the degradation of the instrument, the
    exposure reduces over time. \textit{Images courtesy of NASA.}}
  \label{fig:aia_degradation}
\end{figure}


\begin{figure}[t]%
  % \centering
  \subfigure[]{%
    \label{fig:aia_percentiles}
    \includegraphics[width=\linewidth]{AIA_percentiles.png}%
  }%
  \qquad
  \subfigure[]{%
    \label{fig:aia_outliers}%
    \includegraphics[width=\linewidth]{AIA_outliers.png}% 
  }%
  \qquad
  \subfigure[]{%
    \label{fig:aia_no_degradation}%
    \includegraphics[width=\linewidth]{AIA_no_degradation.png}% 
  }%

  \caption[]{\subref{fig:aia_percentiles} The percentiles of AIA data for images
  taken every 12 hours between May 2010 and December 2020. Due to the large
  range of data, the 25th to 100th percentiles where plotted on a log scale.
  \subref{fig:aia_outliers} The 75th percentile of the AIA data. A simple
  threshold was used to remove poor quality data.}
  \label{fig:aia_data_prep}
\end{figure}


\begin{figure}[t]%
  % \centering
  \subfigure[]{%
    \label{fig:stereo_percentiles}
    \includegraphics[width=\linewidth]{STEREO_percentiles.png}%
  }%
  \qquad
  \subfigure[]{%
  \label{fig:stereo_shifted}
  \includegraphics[width=\linewidth]{STEREO_shifted.png}%
}%
\qquad
\subfigure[]{%
\label{fig:stereo_outliers}
\includegraphics[width=\linewidth]{STEREO_outlier.png}%
}%
\qquad
\subfigure[]{%
\label{fig:stereo_no_degradation}
\includegraphics[width=\linewidth]{STEREO_no_degradation.png}%
}%
  
  \caption[]{}
  \label{fig:stereo_data_prep}
\end{figure}


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{AIA_STEREO_normalised.png}
  \caption{Comparison of the SDO and STEREO data after normalisation.}
  \label{fig:sdo_stereo_comparison}
\end{figure}



\subsection{Magnetogram and Seismic Data}
Figure \ref{fig:hmi_p} shows the percentiles of pixel values for the SDO
magnetogram data. Each pixel on a magnetogram image measures the average radial
magnetic field ($\mathbf{B}\cdot\mathbf{\hat{r}}$) in units of Gauss (G) on the
surface of the sun subtended by the pixel. Similarly, Figure \ref{fig:seismic_p}
shows the percentiles of pixel values for the seismic maps. As explained in
Section \ref{sec:HSM}, seismic maps measure the relative phase shift experienced
by p-modes as they travel to and from the solar farside. \\

Fortunately neither of these datasets exhibited the instrument degredation or
image saturation seen in the EUV data. The magnetogram data was normalised by
dividing it by the absolute maximum pixel value across all the data, which in
this case was $\SI[]{5847.6}[]{G}$, limiting the pixel values to between $-1$
and $1$. Importantly this process is completely reversible, with information
loss only from rounding errors. The raw seismic data had a range between
\SI{-0.9}{Rad} and \SI{0.8}{Rad}. As such normalising this data was deemed
unnecessary. %TODO: explain WHY we need the normalisation
With our data prepared and normalised we now turn to training each of the
image-to-image cGANs.

\begin{figure}[t]%
  % \centering
  \subfigure[]{%
    \label{fig:hmi_p}
    \includegraphics[width=\linewidth]{HMI_percentiles.png}%
  }%
  \\
  \subfigure[]{%
  \label{fig:seismic_p}
  \includegraphics[width=\linewidth]{phase_percentiles.png}%
}%
  \caption[]{\subref{fig:hmi_p} shows the percentiles of the pixel values for
  the magnetogram data. \subref{fig:seismic_p} shows the percntile of pixel
  values for the seismic data.}
  \label{fig:hmi_seismic_p}
\end{figure}


%
%
%
%
%
%
%
%
\chapter{Training}
\label{chap:training}
%
%
%
%
%
%
%
%
With our data processing done, we are finally ready to begin training our deep
neural networks.
% - both the UV GAN and seismic GAN
%TODO make it a story.
%TODO fix this -- maybe introduce UV GAN and Seismic GAN earlier.
We require two deep neural networks in order to generate farside magentgorams
from seismic maps. The first of these must generate magnetograms from EUV
\SI{304}{\angstrom} images, which is then used to generate `STEREO-Magnetograms'
from our STEREO-A EUV data. The second network must then generate magnetograms
from seismic maps. In both cases we use an image-to-image cGAN, which has proven
to be very effective at image-to-image translation. We henceforth refer to these
two cGANs as the `UV-GAN' and the `Seismic GAN' respectively. As outlined in
Section \ref{sec:cgan}, each of these cGANs actually consist of two competing
networks: a `generator' and a `discriminator'. The same generator and
discrimnator architecture is used for both the UV-GAN and the Seismic-GAN. In
this chapter we outline the architecture used for the UV and Seiscmic GAN and
discribe how these deep neural networks were trained.



% \begin{itemize}
%   \item the process for creating a network capable of producing farside
%   magnetograms from seismic maps consists of three parts.
%   \begin{enumerate}
%     \item The UV GAN is trained to generate magnetograms from uv images by using
%     SDO UV/Magnetogram image pairs
%     \item The now trained UV GAN is used to generate `STEREO' magnetograms from
%     STEREO UV images
%     \item The Seismic GAN is trained to generate magnetograms from Seismic
%     images using STEREO magnetogram/ farside seismic image pairs.
%   \end{enumerate}
  
%   \item The same network model was used for parts 1. and 3., which consisted of
%   a U-NET style generator, and a fully convolutional discriminator. Figure
%   \ref{fig:solar_gans_diagram} shows a diagram of both GANs. This architecture
%   for each GAN was based on the one used by \citet{Kim2019} for a very similar
%   purpose.
% \end{itemize}

\begin{figure}
  \centering
  %TODO: this figure. see data analysis book for sketch. maybe different colours
  %for training gan/training
  \caption{Diagram of the UV-GAN and Seismic GAN.}
  \label{fig:solar_gans_diagram}
\end{figure}

%
%
%
%
%
\section{Architecture}
%
%
%
%
%
%

The architecture for each cGAN was based on the one used by \citet{Kim2019}. In
their paper they describe a cGAN similar to the UV-GAN we train which generates
magnetograms from EUV solar images. However while the cGAN used by
\citet{Kim2019} was only capable of predicting magnetic field strengths of at
most 100 G, ours does not have this issue. Our model consists of a U-net style generator network
\citep{ronneberger_u-net_2015}, and a fully convolutional discriminator network. Figure
\ref{fig:solar_gans_diagram} shows a diagram of each cGAN.


\subsection{Generator}
The generator network must be capable of tranlating the conditional image
(either an EUV image or seismic map) into a magnetogram. For this purpose we
chose a U-net \citep{ronneberger_u-net_2015}. U-nets were originally developed
for biomedical image segmentation, however have been used in a wide range of
Astrophysics applications (for example
\citet{felipe_improved_2019,bekki_quantifying_2021,baso_solar_2019}). U-nets
consist of a down sampling path where the width and hight of each layer are
reduced at each step, followed by and upsampling path where the width and height
are get larger at each step until reaching the original size. Many `skip
connections' join layers of the same size either side of the `U'. The model used
in this work is shown in Figure \ref{fig:gen_model}. By implemeting a U-net the
generator is can perform image-to-image translation that retains the shape and
large scale structures of the input image while still capturing complex
relationships between the input and output. While the generator network must be
able to translate between image types, the discriminator network must be able to
evaluate the quality of it's input.

  \begin{figure}
    \centering
    %TODO: fill out figure
    \caption{
      In our case, we start with a ($1024\times 1024$) input image and at each step
      in the down sampling path, apply convolution, batch normalisation and leaky
      reLU activation.% TODO need to explain these
      At each convoluitonal step an additional 64 filters are used than on the
      previous layer, maxing out at 512. The `bottom' layer 
    }
    \label{fig:gen_model}
  \end{figure}


\subsection{Discriminator}
The discrimnator network is given two inputs: a magnetogram (either real or
generated) and the corresponding conditional image - an EUV image for the UV-GAN
or a seismic map for the Seismic-GAN. The network then attempts to determine if
the magnetogram input is real, based on the conditional image. The architecture
of the discrimnator network we used is shown in Figure~\ref{fig:discrim_model}.
The output of the discrimnator is a ($126\times 126$) array, where each element
has a value between 0 and 1. The training objective of the discrimnator network
is to maximise it's output for a true magnetogram input, and minimise it's input
for a generated magnetogram input. A `perfect' descrimnator would then output an
array of only $0$'s for a fake magnetogram input, and an array of $1$'s for a
true magnetogram. On the other hand, the training objective of the generator is
to increase the error-rate of the discrimnator. Informally, the generator can be
thought of trying to `fool' the discrimnator into thinking that the magnetogram
it generated is real. Similar to Section \ref{sec:gan}, the discrimnator's cost
function is given by
\begin{align}
  C_{D(Real)} &= -\mathds{E}\left[\log(D(\mathbf{x}|\mathbf{c}))\right]
\end{align}
for a `true' magnetogram input, and
\begin{align}
  C_{D(Fake)} &=  -\mathds{E}\left[\log(\mathds{1} -  D(G(\mathbf{c})|\mathbf{c}) ) \right]\,,
\end{align}
for a `fake' magnetogram input where $\mathbf{c}$ is the `conditional' input,
$D(\dotsb|\mathbf{c})$ is the discriminator output with a real ($\mathbf{x}$) or
fake ($G(\mathbf{c})$) magnetogram as an input, and $\mathds{1}$ is a tensor
full of $1$'s with the same shape as $D(\dotsb)$. In each expression the `log'
is taken element-wise, before the mean of all tensor elements is taken. The
total discriminator cost function is then
\begin{align}
  C_{D} = C_{D(Real)} + C_{D(Fake)} \,. \label{eqn:d_loss}
\end{align}
\par
As detailed in Section \ref{sec:cgan}, the discriminator also provides the cost
function for the generator. Unlike \ref{sec:cgan} an additional term was added
to minimise the absolute difference between the real and fake magnetograms. With
this addition, the generator cost function used was
\begin{align}
  C_{G} = -\mathds{E}\left[\log(D(G(\mathbf{c})|\mathbf{c}))\right] +
  100 \times \mathds{E}\left[|G(\mathbf{c}) - \mathbf{x}|\right] \,.\label{eqn:G_loss}
\end{align}



\begin{figure}
  \centering
  %TODO: fill out figure
  \caption{
    The input consits of two ($1024\times 1024$) `channels', containing the
    magnetogram (be it real or fake) and the conditional image (either the UV
    image or seismic map depending on the GAN). 5 successive convolutional
    layers with batch normalisation and leaky ReLU activation were applied to
    reduce the final input to a ($126\times 126 \times 1$) tensor.
  }
  \label{fig:discrim_model}
\end{figure}



\section{UV-GAN}
With our architecture specified, we move on to training the UV-GAN. After data
processing we were left with 4247 pairs of normalised SDO EUV and magnetogram
images, captured between April 2010 and December 2019. Images taken in November
and December each year were set aside for evaluation, while the remaining 3505
image pairs were used for training the network. Before training, the weights
(parameters) of the convolutional layers for both the discriminator and
generator where initialised by
\begin{align}
  w_c \sim \mathcal{N}\left(0, 0.02\right) \quad ,
\end{align}
while the weights for batch normalisation were initialised by
\begin{align}
  w_b \sim \mathcal{N}\left(1.0, 0.02\right) \quad .
\end{align}
A kernal (filter) size of $4$ was used for the convoluitonal layers (see Section
\ref{sec:convolutional}). The UV-GAN was trained for $300000$ iterations, with a
batch size of 1, i.e. one magnetogram/EUV image pair per batch. At each
itteration, an EUV image is passed through the generator to produce a fake
magnetogram. The real and fake magnetograms are then both passed through the
discriminator which produces it's output. The parameters of both networks are
then updated using the Adam optimizer \citep{kingma_adam_2014}, according to the
loss functions given by Equations \ref{eqn:d_loss} and \ref{eqn:G_loss}. A
learning rate (step size during gradient descent) of $0.0002$ was used during
the optimisation, with `momentum' parameters $\beta_1 = 0.5$, $\beta_2 = 0.999$.\\
% The small batch size used meant that, while potentially slower and less
% efficient, the GAN will use less memory, and is less likely to result mode
% collapse - where the generator produces the same output regardless of the input.

After the first attempt at training the UV-GAN it was found that it was not able
to reproduce the structure of the active regions. This was thought to be due to
the large dynamic range both the EUV images and magnetograms. This range is
clearly seen in Figures \ref{fig:sdo_stereo_comparison} and \ref{fig:hmi_p},
where which in both cases show that approximately $99\%$ of the pixels in each
image were at least an order of magnitude smaller than the maximum pixel value.
This essentially results in images that are too dark, causing the GAN to largely
focus on the few bright pixels. Previous work have used saturation limits to
deal with this problem by clipping data above a certain point, \citet{Kim2019}
for example used saturation limits of $\pm 100G$ for generating magnetograms.
However this comes at the cost of utility, with the peak magnetic field in many
sunspots exceeding $\SI[]{3000}[]{G}$. To avoid such a cut-off we instead
artificially increased the saturation by amplifying lower intensity pixels. For
the EUV images (both from SDO and STEREO-A) this artificial saturation was done
by taking the square-root of the pixel values. This ensured that the pixel
values remained between the normalised bounds of \SI{0}{DN} and \SI{1}{DN},
while increasing the intensity of the under-represented pixels. For the
magentograms which had pixel values between $\pm \SI{1}{DN}$ this artificial
saturation took the form
\begin{align}
  p^{(\text{new})} = \text{Sign}(p)\sqrt{\absolutevalue{p}}\, ,
\end{align}
amplifying pixels that corresponded to less intense magnetic fields.
Importantly, just as with the normalisation, this process is completely
reversible and the true magnetic field can be easily obtained. Figure
\ref{fig:artificial_sat} shows the percentiles pixel values before and after applying this
artificial saturation. \\


A new cGAN was trained with the same parameters, this time with the artificial
saturation. This time, the UV-GAN was able to produce seemingly realistic
magnetograms, and appear to correctly identify the shape and location of active
regions. Figure \ref{fig:aia_hmi_mag} shows a generated magnetogram along with
the corresponding  The accuracy of these synthetic magentograms will be analysed in
Chapter \ref{chap:results_and_analysis}. \\


Using this trained UV GAN, 5017 synthetic magnetograms where generated between
March 2011 and August 2019 from the corresponding STEREO-A EUV images. We
henceforth refer to these synthetic magnetograms as `STEREO magnetograms'. The
images were chosen such that the time delay between the STEREO-A and farside
images was less than seven days, i.e. STEREO-A was roughly less than one quater
of a solar rotation away from the farside (see Figure \ref{fig:stereo_pos}). A
mask was applied to each of the STEREO magnetograms, setting the value of any
pixels outside the solar disk to zero. Figure \ref{fig:stereo_mag} shows a
STEREO-A \SI{304}{\angstrom} EUV image and the corrosponding synthetic STEREO
magnetogram.


\begin{figure}
  \centering
  
  \caption{Percentiles of UV and magnetogram data before and after the artificial saturation}
  \label{fig:artificial_sat}
\end{figure}



\begin{figure}[t]%
  \includegraphics[width=\linewidth]{aia_hmi_mag.png}
  \caption[]{Taken on the 12th of November 2014. This image was part of the
  testing set and so was not used in training.}
  \label{fig:aia_hmi_mag}
\end{figure}

\begin{figure}[t]
  \centering
  
  \caption{%TODO: STEREO-A EUV and STEREO magnetogram
  }
  \label{fig:stereo_mag}
\end{figure}

\begin{figure}[htbp]
  \centering
  
  \caption{loss}
  \label{fig:uv_loss}
\end{figure}

\section{Seismic-GAN}
\label{sec:train_seismic}

We trained the Seismic-GAN with the same parameters as the UV-GAN using 4288
seismic map/STEREO magnetogram image pairs. Once again images taken from
November or December each year where set aside for evaluation. After this
initial training, the Seismic-GAN was able to produce images that appeared
physically realistic however did not seem to be correlated to the true magnetic
field. This indicates that the cGAN did not actually learn any relationship
between the seismic images and the magnetic field, and only learnt how to
produce an image that `looked' like a magnetogram. An example of one of these
generated magnetograms along with the equivalent STEREO magnetogram is shown is
shown in Figure \ref{fig:default}. \\


%TODO: consider instead making this about decreasing the time delay
%TODO: also mention how many images we used:
%TODO: time delay vs dataset size, talk about trade off
\begin{itemize}
  \item 536 iter/epoch = 4288 images for batch
  \item 2899 for 16 kernal
  \item 4288 for (7 days) default
\end{itemize}

It was hypothesised that due to the 'blurriness' of the seismic images that a
larger filter size should be used in the convolutional layers. It was thought
that this could potentially improve the generated magnetograms allowing the
generator to learn from the large scale structures rather than the small scale
changes.\\

The Seismic-GAN was retrained using a filter size of 16 (as apposed to 4), again
using the same parameters as before. This resulted in mode-collapse, where
the Generator found a local minimum by producing (almost) the same output
image regardless of the input. Figure~\ref{fig:mode_collapse} shows two seismic
maps and the corresponding synthetic magnetograms produced by this cGAN. Despite
the two seismic maps being taken five years apart, both generated magnetograms
appear to be identical.\\

Finally the Seismic GAN was again trained with the smaller filter size, but now
with a batch size of 8 as opposed to 1. This larger batch size means that each
step taken through parameter space will be closer to the optimal step (see
Section \ref{sec:learning}). Figure \ref{fig:batch} shows an example magnetogram
generated using this Seismic GAN. This time the GAN was able to predict some of
the active regions, especially closer to the centre of the image where the
seismic maps are more accurate. However, there was a consistent bias in the
images, and the GAN struggled to detect active regions closer to the edge of the
disk, and often predicted active regions near the edge that didn't exist. We
analyse the performance of this cGAN in Chapter \ref{chap:results_and_analysis}.




\begin{figure}[t]%
  \centering
  \subfigure[]{%
    \label{fig:6.11.11_def_STE}%
    \includegraphics[height=2in]{6.11.11_STE.png}% 
  }%
  \subfigure[]{%
    \label{fig:6.11.11_def_MAG}%
    \includegraphics[height=2in]{6.11.11_MAG.png}% 
  }\\%
  \subfigure[]{%
    \label{fig:6.11.11_def_smap}%
    \includegraphics[height=2in]{6.11.11_smap.png}% 
  }%
  \subfigure[]{%
  \label{fig:6.11.11_default}%
  \includegraphics[height=2in]{6.11.11_default.png}% 
  }%
  \caption[]{Images relating to the first attempt of training the Seismic GAN.
  \subref{fig:6.11.11_def_STE} is a STEREO EUV image taken on 31st of October 2011,
  \subref{fig:6.11.11_def_MAG} is the corresponding magnetogram generated by
  the UV GAN, \subref{fig:6.11.11_def_smap} is the equivalent Siesmic Map taken
  on 6th of November 2011 and
  \subref{fig:6.11.11_default} is the corresponding magnetogram generated by the
  Seismic GAN.}
  \label{fig:default}
\end{figure}


\begin{figure}[t]%
  \centering
  \subfigure[]{%
    \label{fig:6.11.11_ker_smap}%
    \includegraphics[height=2in]{6.11.11_smap.png}% 
  }%
  \subfigure[]{%
  \label{fig:6.11.11_kernal}%
  \includegraphics[height=2in]{6.11.11_16kernal.png}%
  } \\
  \subfigure[]{%
  \label{fig:16.11.06_ker_smap}%
  \includegraphics[height=2in]{16.11.06_smap.png}%
  }%
  \subfigure[]{%
  \label{fig:16.11.06_kernal}%
  \includegraphics[height=2in]{16.11.06_16kernal.png}%
  }%
  \caption[]{\subref{fig:6.11.11_ker_smap} and \subref{fig:6.11.11_kernal}
  show a seismic map taken on the 6th of November 2011 and the corresponding
  magnetogram genreated by the Seismic GAN, after training it with a larger
  kernal size. \subref{fig:16.11.06_ker_smap} shows a seismic map taken 5 years
  later on the 6th of Novermber 2016, while \subref{fig:16.11.06_kernal} shows
  the corresponding generated magnetogram. Despite the different dates and
  seismic maps, the Seismic GAN generates the same magnetogram.}
  \label{fig:mode_collapse}
\end{figure}


\begin{figure}[t]%
  \centering
  \subfigure[]{%
    \label{fig:6.11.11_STE}%
    \includegraphics[height=2in]{6.11.11_STE.png}% 
  }%
  \subfigure[]{%
    \label{fig:6.11.11_MAG}%
    \includegraphics[height=2in]{6.11.11_MAG.png}% 
  }\\
  \subfigure[]{%
    \label{fig:6.11.11_smap}%
    \includegraphics[height=2in]{6.11.11_smap.png}% 
  }%
  \subfigure[]{%
  \label{fig:6.11.11_batch}%
  \includegraphics[height=2in]{6.11.11_batch.png}% 
  }%
  \caption[]{}
  \label{fig:batch}
\end{figure}

\begin{figure}[htbp]
  \centering
  
  \caption{loss}
  \label{fig:seismic_loss}
\end{figure}

%
%
%
%
%
%
%
%
\chapter{Results \& Analysis}
%
%
%
%
%
%
\label{chap:results_and_analysis}

Both the UV-GAN and Seismic-GAN are able to produce magnetograms that appear
physically realistic to the eye. The purpose of generating these magnetograms was
to monitor the level of farside magnetic activity to give some warning of
potential extreme solar events. In this chapter we detail how we can get a
quantitative prediction from these magnetograms, and use these to assess the
validity of our model. 
%TODO: reword maybe

\section{UV-GAN}
By qualitatively inspecting the magnetograms generated by the UV-GAN, (for
example Figure \ref{fig:aia_hmi_mag}) we can see that the cGAN is able to
successfuly reproduce the position and the shape of active regions. Notably
however, it is unable to determine the absolute magnetic field strenghth and
often struggles to reproduce the polarity of individual sunspots, but seems to
guess the polarity in accordance with Hale's law (see Section \ref{sec:dynamo}).
%TODO: reword
To actually determine the usefulness of these magnetograms, we need to develop a
metric for determining the accuracy of our predictions, in particular how
capable it is at predicting extreme magnetic fields. One such metric could be
the average pixel-wise difference between the true and synthetic magnetograms.
This is shown in Figure \ref{fig:uv_loss} for a given batch at each training
iteration. This error is uninterateble however and does not take into account
the relative importance of pixels. For example a pixel near the centre of a
magnetogram will subtend a smaller region of the solar surface than a pixel
closer to the solar limb. This effectively means that the absolute error between
images will be unevenly weighted towards the central pixels. A better metric for
evaluation is the unsigned magnetic flux, \(T_{\text{flux}}\) given by
\begin{align}
  T_{\text{flux}} = \int \int \absolutevalue{B_z} dx dy \, ,
\end{align}
where \(B_z\) is the line-of-sight magnetic field, i.e. the pixel values of the
magnetograms. For indivual active regions, this is typically used as a predictor
for solar flares, for example
\citet{song_statistical_2009,yuan_solar_2010,lan_automated_2012,chen_identifying_2019}.
By comparing the total unsigned magnetic flux of the true SDO magnetograms to
the unsigned magnetic flux of the predicted magnetograms, we evaluate the
accuracy and predictive capability of the synthetic magnetograms. We approximate
the total unsigned magnetic flux by 
\begin{align}
  T_{\text{flux}} \approx \sum\limits_p \absolutevalue{B_z(p)} A(p) \, ,
\end{align}
where $B_z(p)$ is the line-of-sight magnetic field corresponding to pixel $p$
and $A(p)$ is the surface area of the Sun subtended by pixel $p$. Thus, to
calculate the total unsigned magnetic flux we first calculate $A(p)$ for each
pixel.\\

For a given pixel at position \( (x, y) \) in the magnetogram, the equivalent
helioprojective coordinates \((\theta_x, \theta_y)\)are given by
\begin{align}
  \theta_x &= \Delta x (x - c_x) \, \text{, and} \\
  \theta_y &= \Delta y (y - c_y) \, ,
\end{align}
where  \((\Delta x, \Delta y)\) are the angles subtended by the pixel in
arcseconds, and \( (c_x, c_y) \) are the coordinates of the centre of the disk.
Both of these quantaties are available from image metadata. To find the surface
area corresponding to a given pixel we now need to find the coordinates for the
corners of each pixel. These can be found by appropriately adding or subtracting
\(\frac{1}{2} (\Delta x, \Delta y)\). For each of the four corners \((\bm{a},
\bm{b}, \bm{c}, \bm{d})\) of a given pixel, defined such that $\bm{a}$ is
diagonally opposite $\bm{c}$, we find the equivelent heliocentric coordinates
\((x, y, z)\) on the surface of the Sun using Equation
\ref{eqn:heliop_to_helioc}. To approximate the area subtended by a given pixel,
we split these four points into two triangles defined by the vectors
\((\bm{c}-\bm{a}, \bm{b} - \bm{a}) \) and \((\bm{c}-\bm{a}, \bm{d} - \bm{a}) \).
The areas of these triangles can be found according to
\begin{align}
  A_{\text{Triangle}} = \frac{1}{2} \abs*{\bm{v}_1 \times \bm{v}_2} \, ,
\end{align}
where \(\bm{v}_1\), \(\bm{v}_2\) are the vectors defining the triangle. By
summing over the areas of both triangles, we obtain an estimate of the surface
area corresponding to each pixel. Multiplying the area each pixel by the
magnitude of the magnetic field measured for that pixel (i.e. the unsigned pixel
value), we obtain the unsigned the magnetic flux of the pixel. Summing over all
pixels gives us the total unsigned magnetic flux, \(\bm{\phi}\), for that image.
Figure \ref{fig:tumf_calc} shows an example of the unsigned magnetic field of a
magnetogram in addition to the area and unsigned magnetic flux for each pixel.
\\

The unsigned magnetic flux was calculated for each of the SDO and synthetic
magnetograms. Figure \ref{fig:flux_sdo_uv} shows the flux according to the SDO
magnetograms and the UV-GAN using STEREO-EUV data, with vertical lines
indicating X-class solar flares\footnote{Solar flares are classified `X' if the
peak solar flux measured at Earth is greater than \SI{1e-4}{W.m^{-4}}.}.

There is a clear bias between the two predicted fluxes, at
the time of writing the cause of this is unclear.

While this limits the ability of the UV-GAN to predict the absolute strength of
the magnetic field, this is largely not an issue if we can accurately determine
the change in magnetic flux, relative to some fixed point.
%TODO: fix, make plot of relative flux
To this end, the UV-GAN was successful in it's ability to predict peaks and dips
in magnetic flux consistant with the true magnetograms as well as much of the
short and large scale structure. Of particular note, the UV-GAN was able to
reproduce the large-scale trend given by the solar cycle. This is despite
inadvertantly removing some of this information while noramlising the EUV data
(See Section \ref{sec:UV_prep}). Most importantly the UV-GAN was able to predict
the sharp changes in unsigned magnetic flux, including when these sharp changes
corresponded to X-class solar flares. We now move on to the Seismic-GAN. 

%TODO: put in figure

\begin{figure}[t]%
  \centering
  \subfigure[]{%
    \label{fig:tumf_calc_1}%
    \includegraphics[height=2in]{tumf_calc_1.png}% 
  }%
  \subfigure[]{%
    \label{fig:tumf_calc_2}%
    \includegraphics[height=2in]{tumf_calc_2.png}% 
  }%
  \subfigure[]{%
    \label{fig:tumf_calc_3}%
    \includegraphics[height=2in]{tumf_calc_3.png}% 
  }%
  \caption[]{The unsigned magnetic field \subref{fig:tumf_calc_1}, area
  \subref{fig:tumf_calc_2} and unsigned
  magnetic flux \subref{fig:tumf_calc_3} calculated from an SDO magnetogram,
  taken on 5th May, 2010.}
  \label{fig:tumf_calc}
\end{figure}


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{Flux_SDO_UV-GAN_average.png}

  \caption{The total unsigned magnetic flux according
  to SDO (blue) and the UV-GAN using STEREO-A EUV data (orange). The solid line's show the respective
  average over 27 days (approximately one rotation). Solid lines represent the average across 27
  days (roughly one rotational period) while dots represent individual
  magnetograms. The vertical grey lines indicate X-class solar flares, using
  data provided by \url{www.spaceweatherlive.com/}.}
  \label{fig:flux_sdo_uv}
\end{figure}

\section{Seismic GAN}
Qualitatively inspecting the magnetograms, we see that magnetograms produced by
the Seismic GAN appear realistic, i.e. the magnetograms produced have the
characteristics of true magnetograms, with the shapes and polarities appearing
very similar to what you could expect on a true magnetogram. The fine grain
structure does not appear to correlate at all with the true structure however
and the seismic-GAN appears to be `guessing' the details. Futhermore, the
ability of the seismic-GAN to predict the occurance of active regions is mixed
at best, for example in Figure \ref{fig:seismic_2011_11_15} the seismic
magnetogram does correctly identify two active regions, however misses one and
predicts an active region where none exist. While we would have liked these to
be more accurate, the purpose of these magnetograms is to get some indication of
the magnetic activity on the solar farside. To this end, we once again
calculated the total unsigned magnetic flux for each of these synthetic
magnetograms.\\

Figure \ref{fig:flux_sdo_seismic} shows the unsigned magnetic flux of the
magnetograms as a function of time, again with vertical lines indicating X-class
solar flares. As the Seismic-Gan is trained on data from the UV-GAN it is unable
to produce magnetograms more realistic than those of the UV-GAN unless by
chance. As such the magnetic flux corresponding to the Seismic-GAN has the same
bias as with the UV-GAN. While the UV-GAN was able to reproduce much of the
short time scale variations seen in the true magnetic flux, this is not the case
for the Seismic-GAN. This is likely due to the fact that the seismic images are generated by
integrating over a period of ... hours, while the SDO iamges are essentially
instantaneous. %TODO: hours
Additionally, while the UV-GAN was able to reproduce the general shape of the
solar cycle, this long-term variation was much less pronounced for the
Seismic-GAN. Notebly however, most of the solar flares had associated peaks
indicating the potential usefulness of these magnetograms as a predictor of
intense solar activity. Not all peaks corresponded to flares or even high levels
of magnetic activity, for example one of the most prominent peaks near the end
of 2018 came during a period of very low solar activity. Figure
\ref{fig:2018_peak} shows a synthetic farside magnetogram generated during this
time period along with a true SDO nearside magnetogram twelve days later.


\begin{figure}[t]%
  \centering
  \subfigure[]{%
    \label{fig:uv_mag_2011_11_15}%
    \includegraphics[height=1.8in]{UV_MAG_2011_11_15.png}% 
  }%
  \subfigure[]{%
    \label{fig:smap_2011_11_15}%
    \includegraphics[height=1.8in]{smap_2011_11_15.png}% 
  }%
  \subfigure[]{%
    \label{fig:smag_2011_11_15}%
    \includegraphics[height=1.8in]{SMAG_2011_11_15.png}% 
  }%
  \caption[]{}
  \label{fig:seismic_2011_11_15}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{Flux_SDO_Seismic_average.png}
  \caption{The total unsigned magnetic flux according to SDO (blue) and the
  seismic-GAN using Helioseismic data (orange)\protect\footnotemark. Solid lines represent the average across 27
  days (roughly one rotational period) while dots represent individual
  magnetograms. The vertical grey lines indicate X-class solar flares.}
  \label{fig:flux_sdo_seismic}
\end{figure}

\footnotetext{Note that this is different to the figure shown during the
seminar. A bug was discovered on 15/6 which effected how the synthetic
magnetograms from the Seismic-GAN were produced, resulting in different
predictions and a different magnetic flux.}


\begin{figure}[t]%
  \centering
  \subfigure[]{%
    \label{fig:smap_2018peak}%
    \includegraphics[height=1.9in]{smap_2018.10.23.png}% 
  }%
  \subfigure[]{%
    \label{fig:smag_2018peak}%
    \includegraphics[height=1.9in]{SMAG_2018.10.23.png}% 
  }%
  \subfigure[]{%
    \label{fig:hmi_2018peak}%
    \includegraphics[height=1.9in]{HMI_2018.11.05.png}% 
  }%
  \caption[]{The seismic map \subref{fig:smap_2018peak} and synthetic farside
  magnetogram \subref{fig:smag_2018peak} taken on 23/10/2018, corresponding to a
  large peak in the magnetic flux predicted by the Sesimic-GAN. Despite
  predicting intense active regions, none are present on the nearside SDO
  magnetogram \subref{fig:hmi_2018peak} taken on twelve days later.}
  \label{fig:2018_peak}
\end{figure}




%
%
%
%
%
\chapter{Discussion}
\label{chap:discussion}
%
%
%
%
%


With current available data, infering the solar farside magnetic field is a
challenging task. Here we report the ability to generate realistic-looking
magnetograms using only data extracted from nearside dopplergram observations.
While these magnetograms do not appear to accurately represent the farside
magnetic field, they predict an unsigned magnetic flux that peaks during times
of intense - and often flare producing - solar activity.\\

The inherent difficulty of this problem resulted in various limitations to our
method. Perhaps the most obvious such limitation was the lack of any true
farside magnetograms to use as a training set. Overcoming this required
generating synthetic magnetograms based on EUV data. As these synthetic `UV'
magnetograms themselves were not perfect, these provide an upperbound to the
quality of magnetograms generated from farside seismic data, i.e. `seismic'
magnetograms generated from training a cGAN on UV magnetograms will never be
able to outperform the UV magnetograms themselves.
%TODO: reword
In this way the errors compound between training the two cGANs, notably the bias
from UV magnetograms resulted in a similar bias in the seismic magnetograms.
This also means that the Seismic-GAN may learn `quirks' of the UV-GAN, as it's
goal is to match the UV-GAN magnetograms rather than true magnetograms. \\ %reword

Perhaps a bigger limitation however is the level of information about the
magnetic field in the EUV or seismic data. If not enough information is
available to determine the magnetic field, the cGAN's will not be able to
determine the true magnetic field and instead must `guess'. This results in
magnetograms that look realistic, but may not be correlated to the true magnetic
field. This is especially clear in the case of the polarity of individual
sunspots. It is likely that the siesmic disturbance and EUV light do not contain
any information about the magnetic polarity (direction) of a given active
region. It was thought that this information may have been determinable from
context and that the cGAN's may have been able to learn an indirect relationship
between the polarity and the siesmic disturbance/EUV light through the context
of the image - for example Hale's law can be used to predict the polarity of the
leading sunspot (see Section \ref{sec:dynamo}). While it appears that cGAN's
were able to mimic Hale's law to some extent, they were not able to determine
whether active regions deviated from this. It is possible that after more
training the cGAN's may have been able to learn more complex relationships and
some of the underlying physics, but this begins to cut in to the available
resources for training. As it was, fully training a cGAN required roughly four
days of computation time using an expensive GPU. \\


Further restricting the amount of available information was the normalisation of
the EUV data discussed in Section \ref{sec:UV_prep}. As part of this process,
the EUV data was normalised by dividing by a rolling average of the 75th
percentile pixel value. While this mainly affected the background solar
activity, rather than the activity near active regions, this removed information
relating to the general trend of the solar cycle (see Figure
\ref{fig:aia_no_degradation}). To better account for the decrease, the
percentiles could have been fitted with a cominatation of a normal distribution
and exponential decay. By doing this, the the normal distribution would take
into account the effects from the solar cycle while the exponential decay would
capture the instrument degredation - and could be used to adjust for it. This
would have been more difficult to make consistent with the STEREO data however,
which was a necessary step in producing the farside magnetogram training set.
Despite this however, the UV-GAN was able to reproduce the general trend of the
solar cycle as can be seen in Figure \ref{fig:flux_sdo_uv}. \\


As the Seismic-GAN was trained on the synthetic `STEREO magnetograms', a further
limitation came from the timedelay between the seismic maps and STEREO data to
account for the rotation of the Sun. As explained in Section
\ref{sec:data_collection} this timedelay was based on the average rotation of
Sunspots. However due to the differential rotation of the Sun, this still meant
that some active regions may be in different locations after the delay, and more
importantly, active regions may have decayed or new ones emerged during this
period. As the data from STEREO-A was the only viable source of farside
information to compare to the seismic maps, this was unfortunatly necessary.
This creates a trade-off between the time-delay and the amount of data, while
allowing only small time delays gives more accurate data this also restricts the
amount of data. After some trial and error (see Section \ref{sec:train_seismic})
we used data with a time delay of less than seven days. \\


Many of these issues may have been solved if we simply used a neural network to
predict the unsigned magnetic flux for a given seismic image, rather than trying
to generate a magnetogram from scratch. This comes at the cost of
interperability however, generating magnetograms as we did allowed us to
interogate the outputs and understand why the network predicted a given flux.
Perhaps a better method would have been to avoid the use of the EUV data
alltogether, and instead train a cGAN to generate magnetograms from seismic data
based on the nearside images half of a rotation later. While this suffers from
the same issue of emerging and decaying active regions, the shifting of the
active regions would be consistant across the whole dataset. This does not solve
the problem of insufficient informatin however. To overcome this, the cGAN could
be given the magnetogram half a rotation earlier as an additional input. The
cGAN would then only have to learn about changes in the magnetic field rather
than having to produce a magnetogram from scratch. \\




% \begin{itemize}
%   \item weren't happy with the predictive capabilities
%   \item we had magnetograms that we could interogate
%   \item rather than just getting out a number
%   \item would have been much easier to just predict the magnetic flux from the
%   seismic images
%   \item however his comes at the cost of interperability

%   \item both GAN's were flawed - give something that looks realistic but does
%   not necessarily correspond to the true magnetic field
%   \item underlying problem with the methodology itself
%   \item If the GAN does not have enough information, it will make it up, as the
%   discriminator will be unable to determine if the magnetogram is realistic
%   based on the information

%   \item In this way the generator is not required to produce physically possible
% magnetograms that are consistent with Maxwell's equations, it just needs to be
% able to fool the descriminator into believing that

%   \item not required to actually be consistent with maxwell's equations i.e. the
%   generator does not need to produce magnetograms that are physically possible,
%   it just needs to be able to fool the descriminator into thinking that it is.

%   \item it is possible that after more training the cGAN's may have been able to
%   learn more complex relationships and some of the underlying phsyics, but this
%   begins to cut in to the available resources for training. As it was, training
%   a full GAN required roughly four days of computation time using an expensive GPU.

%   \item time delay vs dataset size

%   \item poorly accounted for the long term trend from the degradation
%   \item how we could have done it better
%   \item could potentially improve uv gan by setting a higher lower bound on
%   clipping (we don't need low intensity information). Similar for seismic gan.

%   \item it is likely that the siesmic disturbance and UV light contain any
%   information about the magnetic polarity (direction) of a given active region.
%   \item It was postulated that their might be enough information for the network
%   to learn an indirect relationship between the polarity and the siesmic
%   disturbance/UV light through the context of the image - for example Hale's law
%   can be used to predict the polarity of the leading sunspot.

%   \item trying to find information where there is none
%   \item seismic maps have less data then the magnetograms
%   \item the hope was that the GAN would be able to learn this from context
%   \item i.e. polarity, shape etc 
%   \item for example learning hales law
%   \item part of the problem is that the GAN has to produce a new map every
%   single time
%   \item a better approach could be to use a recurrent neural network, so that
%   the network only has to learn how it changes.
%   \item alternatively could give the gan information about what the nearside did look
%   like before the rotation, and ideally use the Seismic map only to determine
%   what active regions have disapeared, and if any new ones have emerged
  


%   \item It should be noted however that the 75th percentile was still effected
%   by the solar activity, and some of this information was inadvertantly removed in this
%   process.
%   \item this could be the cause of the bias seen in the unsigned flux - gan may
%   have difficulty in determining the background solar activity since this
%   information may have been removed by the normalisation
%   \item to better account for the decrease, could have fitted the percentiles
%   with a normal distribution and exponential decay
%   \item the normal distribution would take into account the solar cycle, while
%   the exponential decay would capture the instrument degredation - and could be
%   used to adjust for it.
%   \item however this would have been more difficult to make consistent with the
%   STEREO data - which was a necessary step in producing the farside magnetogram
%   training set
%   \item an alternative to this could have been to avoid the use of the EUV data
%   alltogether, and instead train the GAN on the nearside images half of a
%   rotation later
%   \item this may have been more difficult to train as many of the active regions
%   will have shifted, however this would consist of a much simpler pipeline.
%   \item another idea could be to give the GAN information about the magnetic
%   field on the nearside before the rotation. The gan would take in information
%   about what the nearside did look like before the rotation, and ideally use the
%   Seismic map only to determine what active regions have disapeared, and if any
%   new ones have emerged
%   \item this would mean the gan would be learning only how the magnetic field
%   changes, and would not have to create a magnetogram from scratch

  

% \end{itemize}

% Experiment with # layers etc

% test on fake images - analyse how it works

% Use a piGAN (physically informed GAN Use a better gan
% openaccess.thecvf.com/content_CVPRW_2020/papers/w11/Alshehhi_Deep_Regression_for_Imaging_Solar_Magnetograms_Using_Pyramid_Generative_Adversarial_CVPRW_2020_paper.pdf

% use recurrent neural network

% do it all in one i.e. nearside dopplergrams -> farside magnetograms

% compare directly to nearside mag rather than farside generated mag

% Could help boundary conditions for dynamo models since large-scale flux is
% generated by the deep-seated dynamo, the observed characteristics of flux
% emergence and that of the subsequent decay provide vital clues as well as
% boundary conditions for dynamo models
% link.springer.com/article/10.1007%2Flrsp-2015-1 

% use smaller gan for seismic GAN (not 1024 by 1024)

% EUV normalisation did remove some of the image information
% however this still retained the overall trend, as seen in Figure (SDO STEREO
% comparison) the characteristic curve of the solar cycle can still be seen



% Copied from earlier:
% \item By using this rotation to our advantage, we can effectively compare
%   farside seismic maps with not-quite-farside STEREO EUV images, albeit with
%   some time delay between images.
%   \item This method isn't perfect however, and has two primary drawbacks:
%   \begin{enumerate}
%     \item The differential rotation of the Sun means that active regions won't
%     necessarily be in the same position after a time delay
%     \item Active regions are constantly changing, for example new active regions may emerge on the surface over the course hours or days,
%     while the decay of sunspots may last from days to weeks
%     \citep{van_driel-gesztelyi_evolution_2015}.
%   \end{enumerate}
%   This will be discussed further in section \ref{chap:discussion}.



\chapter{Conclusion}
A complex self-regenerating dynamo creates magnetic fields on both large and
small scales throughout the Sun. This magnetic field gives rise to active
regions on the surface of the Sun, which can lead to eruptive events such as
flares or coronal mass ejections. Extreme space weather events such as these can
be hazardous to our increasingly technological society, with the potential to
cause mass blackouts, loss of communication, or failure of satellites.
Currently, potentially hazardous active regions can only be identified \(\sim
7\) days before directly facing the Earth due to the rotation of the Sun.
Farside magnetograms offer a potential solution to this, allowing for global
coverage of the solar magnetic field. However, with STEREO no longer giving a
view of the farside, there is currently no reliable method to generate farside
magnetograms. A combination of farside helioseismic holography and deep learning
may present a solution to this through the generation of magnetograms from
seismic maps.



\section{Data availability}

\section{Software}
\begin{itemize}
  \item Python
  \item scikit image \cite{van_der_walt_scikit-image_2014}
  \item numpy
  \item imageio
  \item pandas
  \item tensorflow
  \item PIL
  \item sunpy
  \item astropy
  \item cv2
  \item matplotlib
  \item \url{https://github.com/chemron/honours}
\end{itemize}


\bibliographystyle{mnras.bst}

\bibliography{ml.bib, imaging.bib,space_weather.bib,sun.bib,hsm.bib, Bibliography.bib, unet.bib}


% \appendix


% \chapter{Coordinate Systems}
% For a description of solar coordinate systems, see
% \citet{thompson_w_t_coordinate_2006}.

% \chapter{STEREO orbit}
% \label{app:fun orbit}
% \begin{figure}[ht]
%   \centering
%   \includegraphics[width=0.8\linewidth]{fun_orbit.png}
%   \caption{Trajectory of STEREO spacecraft as it leaves the Earth and uses a gravity assist from the moon to reach it's orbit}
%   \label{fig:fun_orbit}
% \end{figure}

% \chapter{just getting things down quickly}



% \section{STEREO Magnetogram GAN}
% - for the gan the input magnetograms and the seismic maps images needed to have
% the same dimensions - while the seismic maps were 180 by 180 pixels, the
% magnetograms were 1024 by 1024 pixels.

% % currently thinking of upsampling seismic maps then downsampling magnetograms,
% % but commented out paragraph shows my initial idea

% % - to make them the same dimension, the magnetograms were resampled using
% %   bicubic interpolation to the size of the seismic maps. This interpolation
% %   method was chosen as it results in a smoother resampling with fewer
% %   interpolation artifacts \citep{keys_cubic_1981}.


% - to make them the same dimension, the seismic maps were resampled using bicubic
% interpolation to the size of the magnetograms. This interpolation method was
% chosen as it results in a smoother resampling with fewer interpolation artifacts
% \citep{keys_cubic_1981}.



\end{document}

